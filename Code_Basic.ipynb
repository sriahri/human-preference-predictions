{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57477\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             model_a     model_b  \\\n",
       "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
       "1  53567           koala-13b  gpt-4-0613   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[\"I have three oranges today, I ate an orange ...</td>\n",
       "      <td>[\"You have two oranges today.\"]</td>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[\"You are a mediator in a heated political deb...</td>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             prompt  \\\n",
       "0  136060  [\"I have three oranges today, I ate an orange ...   \n",
       "1  211333  [\"You are a mediator in a heated political deb...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                    [\"You have two oranges today.\"]   \n",
       "1  [\"Thank you for sharing the details of the sit...   \n",
       "\n",
       "                                          response_b  \n",
       "0  [\"You still have three oranges. Eating an oran...  \n",
       "1  [\"Mr Reddy and Ms Blue both have valid points ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "print(len(train))\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "print(len(test))\n",
    "\n",
    "display(train[0:2])\n",
    "display(test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train['response_a']=train['response_a']+train['prompt']\n",
    "train['response_b']=train['response_b']+train['prompt']\n",
    "trainA=train[['response_a',\"winner_model_a\"]]\n",
    "trainB=train[['response_b',\"winner_model_b\"]]  \n",
    "trainA.columns=['text','label']\n",
    "trainB.columns=['text','label']\n",
    "train=pd.concat([trainA,trainB],axis=0)\n",
    "train.to_csv(\"new_train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test['response_a']=test['response_a']+test['prompt']\n",
    "test['response_b']=test['response_b']+test['prompt']\n",
    "testA=test[['response_a']]\n",
    "testB=test[['response_b']]  \n",
    "testA.columns=['text']\n",
    "testB.columns=['text']\n",
    "test=pd.concat([testA,testB],axis=0)\n",
    "test.to_csv(\"new_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srihari/.conda/envs/venv-p310-torch210-srihari/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler() \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def random_seed(SEED):\n",
    "    \n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 171\n",
    "random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"As an AI language model, I cannot predict fu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"Yes, \\\"I am happy\\\" is a grammatically corre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"Certainly! Here are a few highly regarded ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"A square and a rectangle are both four-sided...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"Sure, I'll be happy to help with anything yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>[\"The future of AI is a topic of much discussi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>[\"Turkey\"][\"what is the country (name only one...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>[\"Sure, here are some names that might sound u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>[\"I apologize, but I'm a large language model,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>[\"No.\"][\"Respond with 'Yes' if the following t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     [\"As an AI language model, I cannot predict fu...      0\n",
       "1     [\"Yes, \\\"I am happy\\\" is a grammatically corre...      1\n",
       "2     [\"Certainly! Here are a few highly regarded ar...      1\n",
       "3     [\"A square and a rectangle are both four-sided...      1\n",
       "4     [\"Sure, I'll be happy to help with anything yo...      0\n",
       "...                                                 ...    ...\n",
       "4995  [\"The future of AI is a topic of much discussi...      1\n",
       "4996  [\"Turkey\"][\"what is the country (name only one...      0\n",
       "4997  [\"Sure, here are some names that might sound u...      1\n",
       "4998  [\"I apologize, but I'm a large language model,...      1\n",
       "4999  [\"No.\"][\"Respond with 'Yes' if the following t...      0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"You have two oranges today.\"][\"I have three ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"When you want to initialize the classificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[\"To initialize the classification head when p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [\"You have two oranges today.\"][\"I have three ...\n",
       "1  [\"Thank you for sharing the details of the sit...\n",
       "2  [\"When you want to initialize the classificati...\n",
       "3  [\"You still have three oranges. Eating an oran...\n",
       "4  [\"Mr Reddy and Ms Blue both have valid points ...\n",
       "5  [\"To initialize the classification head when p..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=pd.read_csv('new_train.csv')\n",
    "data=data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data=data[0:5000]\n",
    "display(data)\n",
    "\n",
    "TEST=pd.read_csv('new_test.csv')\n",
    "display(TEST)\n",
    "TEST['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[\"The command \\\\\"finger 0@hostname\\\\\" (that is a zero) against a Solaris 8 system should display the information of the user with a username of \\\\\"0\\\\\" and their hostname \\\\\"hostname\\\\\".\"][\"during a penetration test, What would you expect the command \\\\\"finger 0@hostname\\\\\" (that is a zero) against a Solaris 8 system to display?\"]</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_s = train['text'].iloc[0]\n",
    "result1 = tokenizer.encode_plus(test_s)\n",
    "tokenizer.decode(result1[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "result2 = tokenizer.encode_plus(\n",
    "    test_s,\n",
    "    add_special_tokens = True, \n",
    "    max_length = 20, \n",
    "    pad_to_max_length = True, \n",
    "    truncation = True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[\"The command \\\\\"finger 0@hostname\\\\\" (that is a zero) against a</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result2[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "max_sens = 20\n",
    "\n",
    "train = train.sort_values(\"label\").reset_index(drop=True)\n",
    "\n",
    "train[\"kfold\"] = train.index % 5\n",
    "\n",
    "p_train = train[train[\"kfold\"]!=0].reset_index(drop=True)\n",
    "p_valid = train[train[\"kfold\"]==0].reset_index(drop=True)\n",
    "\n",
    "p_test=TEST.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class BERTDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self,sentences,targets):        \n",
    "        self.sentences = sentences\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self,idx):        \n",
    "        sentence = self.sentences[idx]    \n",
    "        bert_sens = tokenizer.encode_plus(\n",
    "                                sentence,\n",
    "                                add_special_tokens = True, \n",
    "                                max_length = max_sens, \n",
    "                                pad_to_max_length = True, \n",
    "                                return_attention_mask = True)\n",
    "\n",
    "        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n",
    "        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n",
    "\n",
    "        target = torch.tensor(self.targets[idx],dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "\n",
    "                'targets': target\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = BERTDataSet(p_train[\"text\"],p_train[\"label\"])\n",
    "valid_dataset = BERTDataSet(p_valid[\"text\"],p_valid[\"label\"])\n",
    "test_dataset = BERTDataSet(p_test[\"text\"],p_test[\"label\"])\n",
    "\n",
    "\n",
    "train_batch = 16\n",
    "valid_batch = 32\n",
    "test_batch = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=8,pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle = False,num_workers=8,pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=test_batch,shuffle = False,num_workers=8,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchviz import make_dot\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "for a in train_dataloader:\n",
    "    ids = a[\"ids\"].to(device)\n",
    "    mask = a[\"mask\"].to(device)\n",
    "    output = model(ids,mask)\n",
    "    make_dot(output.logits, params=dict(model.named_parameters())).render(\"roberta_model_architecture\", format=\"png\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output = output[\"logits\"].squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "LR=2e-5\n",
    "optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 5\n",
    "train_steps = int(len(p_train)/train_batch*epochs)\n",
    "print(train_steps)\n",
    "num_steps = int(train_steps*0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(output,target):\n",
    "    return torch.sqrt(nn.MSELoss()(output,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def training(\n",
    "    train_dataloader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler\n",
    "):\n",
    "    \n",
    "    model.train()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    allpreds = []\n",
    "    alltargets = []\n",
    "\n",
    "    for a in train_dataloader:\n",
    "\n",
    "        losses = []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            ids = a[\"ids\"].to(device,non_blocking=True)\n",
    "            mask = a[\"mask\"].to(device,non_blocking=True)\n",
    "\n",
    "            output = model(ids,mask)\n",
    "            output = output[\"logits\"].squeeze(-1)\n",
    "            target = a[\"targets\"].to(device,non_blocking=True)\n",
    "            loss = loss_fn(output,target)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            allpreds.append(output.detach().cpu().numpy())\n",
    "            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n",
    "\n",
    "        scaler.scale(loss).backward() \n",
    "        scaler.step(optimizer) \n",
    "        scaler.update() \n",
    "        \n",
    "        del loss \n",
    "\n",
    "        scheduler.step() \n",
    "\n",
    "    allpreds = np.concatenate(allpreds)\n",
    "    alltargets = np.concatenate(alltargets)\n",
    "    losses = np.mean(losses)\n",
    "    train_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n",
    "\n",
    "    return losses,train_rme_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def validating(valid_dataloader,model):\n",
    "    \n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alltargets = []\n",
    "\n",
    "    for a in valid_dataloader:\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ids = a[\"ids\"].to(device)\n",
    "            mask = a[\"mask\"].to(device)\n",
    "\n",
    "            output = model(ids,mask)\n",
    "            output = output[\"logits\"].squeeze(-1)\n",
    "            target = a[\"targets\"].to(device)\n",
    "            loss = loss_fn(output,target)\n",
    "            losses.append(loss.item())\n",
    "            allpreds.append(output.detach().cpu().numpy())\n",
    "            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n",
    "            \n",
    "            del loss\n",
    "\n",
    "    allpreds = np.concatenate(allpreds)\n",
    "    alltargets = np.concatenate(alltargets)\n",
    "    losses = np.mean(losses)\n",
    "    valid_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n",
    "\n",
    "    return allpreds,losses,valid_rme_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------0start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.49962965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47541237\n",
      "Save first model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 1/5 [00:26<01:45, 26.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------1start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.48293564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:44<01:04, 21.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.48266506\n",
      "---------------2start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.48006925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47406197\n",
      "found better point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:10<00:47, 23.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------3start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.47994658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [01:29<00:21, 21.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47455174\n",
      "---------------4start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.47555006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.4730047\n",
      "found better point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:55<00:00, 23.08s/it]\n"
     ]
    }
   ],
   "source": [
    "trainlosses = []\n",
    "vallosses = []\n",
    "bestscore = None\n",
    "trainscores = []\n",
    "validscores = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    print(\"---------------\" + str(epoch) + \"start-------------\")\n",
    "    \n",
    "    trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)    \n",
    "    trainlosses.append(trainloss)\n",
    "    trainscores.append(trainscore)\n",
    "    \n",
    "    print(\"trainscore is \" + str(trainscore))\n",
    "    \n",
    "    preds,validloss,valscore=validating(valid_dataloader,model)    \n",
    "    vallosses.append(validloss)\n",
    "    validscores.append(valscore)\n",
    "    \n",
    "    print(\"valscore is \" + str(valscore))\n",
    "    \n",
    "    if bestscore is None:\n",
    "        bestscore = valscore\n",
    "        \n",
    "        print(\"Save first model\")\n",
    "        \n",
    "        state = {\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer_dict': optimizer.state_dict(),\n",
    "                        \"bestscore\":bestscore\n",
    "                    }\n",
    "            \n",
    "        torch.save(state, \"model0.pth\")\n",
    "        \n",
    "    elif bestscore > valscore:\n",
    "        \n",
    "        bestscore = valscore        \n",
    "        print(\"found better point\")        \n",
    "        state = {\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer_dict': optimizer.state_dict(),\n",
    "                        \"bestscore\":bestscore\n",
    "                    }\n",
    "            \n",
    "        torch.save(state, \"model0.pth\")\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB4mklEQVR4nO3deZxcZZn3/899llq6ekunsxOSEAIBRfJIDA+IEDVORlBGHCUoQogLww8VNYKiDJuIUcZhogya0QFRcEERV3hAjaCiURSIqEBESAghezq9Vddylvv3R3U3aZKQSkidTle+79erXkmduurUXaei5+JerttYay0iIiIiBxFnuBsgIiIikjQlQCIiInLQUQIkIiIiBx0lQCIiInLQUQIkIiIiBx0lQCIiInLQUQIkIiIiBx0lQCIiInLQUQIkIiIiBx0lQCIHgDVr1mCM4ZZbbhk8dtVVV2GMqer9xhiuuuqq/dqmuXPnMnfu3P16zpHihd99V7/PSzV16lTOO++8/Xa+elKLf88iL6QESGQvnX766TQ0NNDT07PbmLPPPptUKsW2bdsSbNnee+yxx7jqqqtYs2bNcDdl0P33348xZvDh+z6HHXYY5557Lk8//fRwN2+v/O53v+Oqq66is7NzuJsy6JZbbhlyfT3PY9KkSZx33nk899xzw928XToQr6OMfEqARPbS2WefTaFQ4Ac/+MEuX+/r6+NHP/oR//zP/8zo0aP3+XP+/d//nUKhsM/vr8Zjjz3G1VdfvcsE6Gc/+xk/+9nPavr5L+aiiy7i1ltv5Stf+QqnnXYat99+O6961atYv3594m2ZMmUKhUKBc845Z6/e97vf/Y6rr756lzfuVatW8dWvfnU/tXDvfepTn+LWW29l2bJlvPGNb+S2227jlFNOoVgsDlubdufFrqPIvlICJLKXTj/9dJqamvjWt761y9d/9KMfkc/nOfvss1/S53ieRyaTeUnneClSqRSpVGrYPv81r3kN73rXu1i0aBE33HADn//85+no6ODrX//6bt+Tz+dr0hZjDJlMBtd199s50+k0vu/vt/PtrTe+8Y28613v4r3vfS//+7//y8UXX8xTTz3Fj3/842Frk0iSlACJ7KVsNstb3/pWli9fzubNm3d6/Vvf+hZNTU2cfvrpdHR0cPHFF3PMMcfQ2NhIc3Mzb3zjG/nzn/+8x8/Z1RygUqnERz7yEcaMGTP4GevWrdvpvc888wwXXnghRx55JNlsltGjR/P2t799SE/PLbfcwtvf/nYAXvva1w4Oidx///3ArucAbd68mfe85z2MGzeOTCbDscceu1NCMjBf5vOf/zxf+cpXmD59Oul0mle96lX88Y9/3OP33p3Xve51AKxevXrI9Xnsscd45zvfyahRozjppJMG42+77TaOO+44stksbW1tnHXWWTz77LM7nXegjdlsljlz5vCb3/xmp5jdzQF64oknOPPMMxkzZgzZbJYjjzySyy67bLB9l1xyCQDTpk0bvL4Dv8Gu5gA9/fTTvP3tb6etrY2Ghgb+7//9v9x1111DYgaGCL/73e9y7bXXcsghh5DJZHj961/PP/7xj+ov6Au85jWvAeCpp57a6Tu+7W1vo62tjUwmw+zZs3dKkoIg4Oqrr2bGjBlkMhlGjx7NSSedxM9//vPBmN3NKTvvvPOYOnXqbtu1p+sosq+84W6AyEh09tln8/Wvf53vfve7fOADHxg83tHRwb333ss73vEOstksf/vb3/jhD3/I29/+dqZNm8amTZv4n//5H0455RQee+wxJk6cuFef+973vpfbbruNd77znZx44on88pe/5LTTTtsp7o9//CO/+93vOOusszjkkENYs2YNX/7yl5k7dy6PPfYYDQ0NnHzyyVx00UV88Ytf5JOf/CRHHXUUwOCfL1QoFJg7dy7/+Mc/+MAHPsC0adP43ve+x3nnnUdnZycf+tCHhsR/61vfoqenh3/7t3/DGMN1113HW9/6Vp5++ul96vkYuDG/cFjx7W9/OzNmzOAzn/kM1loArr32Wi6//HLOPPNM3vve97JlyxZuuOEGTj75ZB555BFaW1sBuOmmm/i3f/s3TjzxRD784Q/z9NNPc/rpp9PW1sbkyZNftD2PPvoor3nNa/B9n/PPP5+pU6fy1FNP8ZOf/IRrr72Wt771rfz973/n29/+Nv/1X/9Fe3s7AGPGjNnl+TZt2sSJJ55IX18fF110EaNHj+brX/86p59+OnfccQdnnHHGkPjPfvazOI7DxRdfTFdXF9dddx1nn302f/jDH/b62gKDCcWoUaMGj/3tb3/j1a9+NZMmTeLSSy8ll8vx3e9+l7e85S18//vfH2zTVVddxZIlS3jve9/LnDlz6O7u5k9/+hMPP/wwb3jDG/apPQP29jqKVM2KyF4Lw9BOmDDBnnDCCUOOL1u2zAL23nvvtdZaWywWbRRFQ2JWr15t0+m0/dSnPjXkGGC/9rWvDR678sor7Y7/E125cqUF7IUXXjjkfO985zstYK+88srBY319fTu1ecWKFRaw3/jGNwaPfe9737OAve+++3aKP+WUU+wpp5wy+Hzp0qUWsLfddtvgsXK5bE844QTb2Nhou7u7h3yX0aNH246OjsHYH/3oRxawP/nJT3b6rB3dd999FrA333yz3bJli12/fr2966677NSpU60xxv7xj38ccn3e8Y53DHn/mjVrrOu69tprrx1y/C9/+Yv1PG/weLlctmPHjrWzZs2ypVJpMO4rX/mKBYZ89139PieffLJtamqyzzzzzJDPieN48O//8R//YQG7evXqnb7nlClT7MKFCweff/jDH7aA/c1vfjN4rKenx06bNs1OnTp18N/RwPU56qijhrT7C1/4ggXsX/7yl11d1kFf+9rXLGB/8Ytf2C1btthnn33W3nHHHXbMmDE2nU7bZ599djD29a9/vT3mmGNssVgc8v1OPPFEO2PGjMFjxx57rD3ttNNe9HNf+O9pwMKFC+2UKVOGHHvhv+cXu44i+0pDYCL7wHVdzjrrLFasWDGkK/5b3/oW48aN4/Wvfz1QmefhOJX/mUVRxLZt22hsbOTII4/k4Ycf3qvPvPvuu4HK5OAdffjDH94pNpvNDv49CAK2bdvG4YcfTmtr615/7o6fP378eN7xjncMHvN9n4suuoje3l5+9atfDYlfsGDBkN6EgSGWaldyvfvd72bMmDFMnDiR0047jXw+z9e//nVmz549JO6CCy4Y8vzOO+8kjmPOPPNMtm7dOvgYP348M2bM4L777gPgT3/6E5s3b+aCCy4YMtfpvPPOo6Wl5UXbtmXLFn7961/z7ne/m0MPPXTIa9WWLnihu+++mzlz5gwZxmtsbOT8889nzZo1PPbYY0PiFy1aNKTde3t9582bx5gxY5g8eTJve9vbyOVy/PjHP+aQQw4BKr2Zv/zlLznzzDPp6ekZvI7btm1j/vz5PPnkk4OrxlpbW/nb3/7Gk08+uU/fXWQ4KAES2UcDk5wHJkOvW7eO3/zmN5x11lmDk2XjOOa//uu/mDFjBul0mvb2dsaMGcOjjz5KV1fXXn3eM888g+M4TJ8+fcjxI488cqfYQqHAFVdcweTJk4d8bmdn515/7o6fP2PGjMGEbsDAkNkzzzwz5PgLE4OBZGj79u1Vfd4VV1zBz3/+c375y1/y6KOPsn79+l2uwpo2bdqQ508++STWWmbMmMGYMWOGPB5//PHBeVsD7Z0xY8aQ9w8su38xA0nGy1/+8qq+SzWeeeaZXf6Wtbq+N954Iz//+c+54447OPXUU9m6dSvpdHrw9X/84x9Ya7n88st3uo5XXnklwOC1/NSnPkVnZydHHHEExxxzDJdccgmPPvpold9cZHhoDpDIPjruuOOYOXMm3/72t/nkJz/Jt7/9bay1Q1Z/feYzn+Hyyy/n3e9+N9dccw1tbW04jsOHP/xh4jiuWds++MEP8rWvfY0Pf/jDnHDCCbS0tGCM4ayzzqrp5+5odyumbP88nT055phjmDdv3h7jduztgkrSaYzh//2//7fLNjQ2Nlb1+Qe6l3p958yZM9ib9pa3vIWTTjqJd77znaxatYrGxsbBfycXX3wx8+fP3+U5Dj/8cABOPvlknnrqKX70ox/xs5/9jP/93//lv/7rv1i2bBnvfe97gUrP2K7aFkVRVe0V2d+UAIm8BGeffTaXX345jz76KN/61reYMWMGr3rVqwZfv+OOO3jta1/LTTfdNOR9nZ2dg5M5qzVlyhTiOOapp54a0lOwatWqnWLvuOMOFi5cyH/+538OHisWizvVUdmb4ZopU6bw6KOPEsfxkF6gJ554YvD1A8H06dOx1jJt2jSOOOKI3cYNtPfJJ58cXGEGlSHD1atXc+yxx+72vQM9RH/9619ftC17e3139VsmcX1d12XJkiW89rWv5b//+7+59NJLB7+j7/tVJaJtbW0sWrSIRYsW0dvby8knn8xVV101mACNGjVql8NzL+zZ2pV9HVYUeTEaAhN5CQZ6e6644gpWrly5U+0f13V3+q/e733ve/tUcfeNb3wjAF/84heHHF+6dOlOsbv63BtuuGGn/9rO5XIAVRWYO/XUU9m4cSO333774LEwDLnhhhtobGzklFNOqeZr1Nxb3/pWXNfl6quv3ukaWGsHq3PPnj2bMWPGsGzZMsrl8mDMLbfcssfrMWbMGE4++WRuvvlm1q5du9NnDNjb6/vggw+yYsWKwWP5fJ6vfOUrTJ06laOPPnqP53gp5s6dy5w5c1i6dCnFYpGxY8cyd+5c/ud//ocNGzbsFL9ly5bBv7+w4nljYyOHH344pVJp8Nj06dN54oknhrzvz3/+M7/97W/32La9uY4i1VIPkMhLMG3aNE488UR+9KMfAeyUAL3pTW/iU5/6FIsWLeLEE0/kL3/5C9/85jf3OMdkV2bNmsU73vEOvvSlL9HV1cWJJ57I8uXLd1n75U1vehO33norLS0tHH300axYsYJf/OIXOy0hnzVrFq7r8rnPfY6uri7S6TSve93rGDt27E7nPP/88/mf//kfzjvvPB566CGmTp3KHXfcwW9/+1uWLl1KU1PTXn+nWpg+fTqf/vSn+cQnPsGaNWt4y1veQlNTE6tXr+YHP/gB559/PhdffDG+7/PpT3+af/u3f+N1r3sdCxYsYPXq1Xzta1+r6vf54he/yEknncQrX/lKzj//fKZNm8aaNWu46667WLlyJVAZJgW47LLLOOuss/B9nze/+c2DN/QdXXrppXz729/mjW98IxdddBFtbW18/etfZ/Xq1Xz/+9/fae5VLVxyySW8/e1v55ZbbuGCCy7gxhtv5KSTTuKYY47hfe97H4cddhibNm1ixYoVrFu3brCe1dFHH83cuXM57rjjaGtr409/+hN33HHHkBIR7373u7n++uuZP38+73nPe9i8eTPLli3jZS97Gd3d3S/arr25jiJVG57FZyL148Ybb7SAnTNnzk6vFYtF+9GPftROmDDBZrNZ++pXv9quWLFipyXB1SyDt9baQqFgL7roIjt69Giby+Xsm9/8Zvvss8/utGx4+/btdtGiRba9vd02Njba+fPn2yeeeGKnpdfWWvvVr37VHnbYYdZ13SFL4ne1bHnTpk2D502lUvaYY44Z0uYdv8t//Md/7HQ9XtjOXRlY5v29733vReMGrs+WLVt2+fr3v/99e9JJJ9lcLmdzuZydOXOmff/7329XrVo1JO5LX/qSnTZtmk2n03b27Nn217/+dVW/j7XW/vWvf7VnnHGGbW1ttZlMxh555JH28ssvHxJzzTXX2EmTJlnHcYYs5d7Vb/HUU0/Zt73tbYPnmzNnjv3pT39a1fXZXRtfaGAZ/EA5gR1FUWSnT59up0+fbsMwHGzTueeea8ePH29937eTJk2yb3rTm+wdd9wx+L5Pf/rTds6cOba1tdVms1k7c+ZMe+2119pyuTzk/Lfddps97LDDbCqVsrNmzbL33ntvVcvgX+w6iuwrY22VM+ZERERE6oTmAImIiMhBRwmQiIiIHHSUAImIiMhBRwmQiIiIHHSUAImIiMhBRwmQiIiIHHRUCHEX4jhm/fr1NDU1qQS7iIjICGGtpaenh4kTJ+6xeKgSoF1Yv349kydPHu5miIiIyD549tlnOeSQQ140RgnQLgyU9H/22Wdpbm4e5taIiIhINbq7u5k8eXJVW/MoAdqFgWGv5uZmJUAiIiIjTDXTVzQJWkRERA46SoBERETkoKMESERERA46SoBERETkoKMESERERA46SoBERETkoKMESERERA46SoBERETkoKMESERERA46qgSdIGst+XJEGMV4rkMu5WqzVRERkWGgBCghXYWAZ7bl6egtE8YWzzG0NaaYMjpHS9Yf7uaJiIgcVJQAJaCrEPDX57rIl0JGNaRIeQ7lMGZjV5GeYsjLJ7UoCRIREUmQ5gDVmLWWZ7blyZdCJrRkyfgujjFkfJcJLVnypZC1HXmstcPdVBERkYOGEqAay5cjOnrLjGpI7fL1UQ0ptvWUyZejhFsmIiKSLGstvaWQzr4yvaVwWP/jX0NgNRZGMWFsSXm7zjV91yGMLWEUJ9wyERGR5Bxoc2GVANWY5zp4jqEcxmR8d6fXgyjGcwyeq844ERGpTwfiXFjddWssl3Jpa0yxva+8y9e395UZ3ZQil9o5ORIRERnpDtS5sEqAaswYw5TROXJpjw1dBYpBRBRbikHEhq4CubTHoW051QMSEZG6dKDOhdUQWAJasj4vn9Sy09jnhNYMh7apDpCIiNSvA3UurBKghLRkfY6Z1KJK0CIiclA5UOfCaggsQcYYGtMerQ0pGtOekh8REal7B+pcWCVAIiIiUjMH6lxYDYGJiIhITR2Ic2GVAImIiEjNHWhzYZUAiYiISCIG5sIeCDQHSERERA46SoBERETkoKMESERERA46SoBERETkoKMESERERA46SoBERETkoKMESERERA46SoBERETkoHNAJEA33ngjU6dOJZPJcPzxx/Pggw9W9b7vfOc7GGN4y1veMuS4tZYrrriCCRMmkM1mmTdvHk8++WQNWi4iIiIj0bAnQLfffjuLFy/myiuv5OGHH+bYY49l/vz5bN68+UXft2bNGi6++GJe85rX7PTaddddxxe/+EWWLVvGH/7wB3K5HPPnz6dYLNbqa4iIiMgIMuwJ0PXXX8/73vc+Fi1axNFHH82yZctoaGjg5ptv3u17oiji7LPP5uqrr+awww4b8pq1lqVLl/Lv//7v/Mu//AuveMUr+MY3vsH69ev54Q9/WONvIyIiIiPBsCZA5XKZhx56iHnz5g0ecxyHefPmsWLFit2+71Of+hRjx47lPe95z06vrV69mo0bNw45Z0tLC8cff/xuz1kqleju7h7yEBERkfo1rAnQ1q1biaKIcePGDTk+btw4Nm7cuMv3PPDAA9x000189atf3eXrA+/bm3MuWbKElpaWwcfkyZP39qtUxVpLbymks69MbynEWluTzxEREZEXd2BsyVqlnp4ezjnnHL761a/S3t6+3877iU98gsWLFw8+7+7u3u9JUFch4JlteTp6y4SxxXMMbY0ppozO0ZL19+tniYiIyIsb1gSovb0d13XZtGnTkOObNm1i/PjxO8U/9dRTrFmzhje/+c2Dx+I4BsDzPFatWjX4vk2bNjFhwoQh55w1a9Yu25FOp0mn0y/16+xWVyHgr891kS+FjGpIkfIcymHMxq4iPcWQl09qURIkIiKSoGEdAkulUhx33HEsX7588FgcxyxfvpwTTjhhp/iZM2fyl7/8hZUrVw4+Tj/9dF772teycuVKJk+ezLRp0xg/fvyQc3Z3d/OHP/xhl+esNWstz2zLky+FTGjJkvFdHGPI+C4TWrLkSyFrO/IaDhMREUnQsA+BLV68mIULFzJ79mzmzJnD0qVLyefzLFq0CIBzzz2XSZMmsWTJEjKZDC9/+cuHvL+1tRVgyPEPf/jDfPrTn2bGjBlMmzaNyy+/nIkTJ+5ULygJ+XJER2+ZUQ2pXb4+qiHFtp4y+XJEY3rYfw4REZGDwrDfcRcsWMCWLVu44oor2LhxI7NmzeKee+4ZnMS8du1aHGfvOqo+9rGPkc/nOf/88+ns7OSkk07innvuIZPJ1OIrvKgwigljS8rb9XfwXYcwtoRRnHDLREREDl7GauxlJ93d3bS0tNDV1UVzc/NLOldvKeRPqzvIpT0yvrvT68UgIl8KmT2tTT1AIiIiL8He3L+HvRBivculXNoaU2zvK+/y9e19ZUY3pcildk6OREREpDaUANWYMYYpo3Pk0h4bugoUg4gothSDiA1dBXJpj0PbchhjhrupIiIiBw2NuSSgJevz8kktO9UBmtCa4dA21QESERFJmhKghLRkfY6Z1EK+HBFGMZ7rkEu56vkREREZBkqAEmSM0URnERGRA4DmAImIiMhBRwmQiIiIHHSUAImIiMhBRwmQiIiIHHSUAImIiMhBR0uSREREJBHW2gOmHIwSoAQdSD+8iIhIkroKAWu29rK+s0gpjEl7DhNbM0xtbxyWgsBKgBLSVQh2qgTd1phiymhVghYRkfrWVQj4w9PbWLMtj7XgGIgtPLu9j03dJY4/bHTi90LNAUpAVyHgr891saGzgGMMGd/BMYYNnQX++lwXXYVguJsoIiJSE9Za/ra+i8c3dIM1tGR92nLpSsJjDY9v6OaxDV1YaxNtlxKgGrPW8sy2PFt6ivSVI57e0ssTG3t4eksvfeWILT1F1nbkE//hRUREktBbClm1oQfPMYxpSpP2XBxjSHsuY5rSeI5h1foeekthou1SAlRj+XLEs9v66MiX2dxdwhhIew7GwObuEh35Mmu39pEvR8PdVBERkf2uuxDQWSgzKpfa5eutDSm2F8p0JzwaojlANRaEERu6iuTLIQbY2lsithbHGJoyHsUQYlskCCPQPmEiIlKPXmSQw5gXf71WdMetsXJk6ciX6CtHmP5f2BpDHMdsz0dYDKUgohxpCExEROpPc9antcFne75MpsWlFMZEscV1DGnPYXu+zKicT3PCk6CVANWY70AxjNjUVcAa6OoLiWyMaxxaGjywMGlUFl+DkSIiUoca0x4zJ7Tw679v5i/rOgkiOzgS4ruGTMrllVPaaEx4FEQJUI0FMZRDy6beEkFkacumSHku5dDyXGcR3zW0N2UI4uFuqYiIyP5njOGQtizGGJ7ZnocYGBj2cmDm+JbB15OkBKjGPGPpLpYxBia1ZikGEeXYYpxKz8+WniK9pQDPaAhMRETqj7WWdR0FrIVD23IEUYy1lbk/KdfBWljXUeCQ1mSTICVANdZVjIgiGNNYWerXlPGw9Ce/tnI8CC1dxYjRTcPdWhERkf2rtxTyxIYuGtMuM8Y2UgojIguugbTnsqGrwKqNXbx8UjNNmeTmASkBqjHHQC7tEVlLvhTsNAeoIeXjOQZHO2KIiEgd6i4EdPYFjGvJ7PL1UbkUm7qKdBcCJUD1JJvyaMq4PNtRIIxiWhs8XGOIrKUUxMRxwOS2LNmUfgoREalTBopBzNaeEr3FkAiLi6Ex45FL+5VhkYRp7VGNtfcv7SuEISnPpbMvZHNvic6+kJTvUghDWhtStOe0H5iIiNSf5qxP2nV4YmMX2/sCjAHfrRQE3t4XsGpTFxnP0TL4elMILa25FL7r0dkX0Nbo43suQRjR0RvQkPFobvAphJZGd7hbKyIisn/lUi4tDT7dzwUUy1FlArQDNq5MhC5HMS0NKXKpZG+CSoBqLAgjymHMzHGNdBYCOnrL5EshnjEcPi5HU8YnCGNVghYRkbrUF8SkPJemTIqt3UVwqSyFd8DEMLopg+869AUxjenkBqZ0x62xcmQplCLGt2SZPraRnmJIEMb4nkNTxiNfiujsK6sStIiI1KUgjOgslGnKuHT2wcauEuXIknINE1rSNGZcugrlxDsClADVWMo1NKQ8CkFEY9oj7bl4joPrGAyGQhDRkPZIuVoGJiIi9accWTZ0FVm9tY+evgDXdck6lUJAHb0BQdyHMSbxjgAlQDXmey4TWjI805Hn0Wc7CazFxhbjGHxjGNuS5pDWHL6nCUAiIlJ/PGNZv73As9vyOI5DGMVYYgwOnuuwvZAn4zmJFwRWAlRjuZTLqMYUD6/tYFtfiUI5gv5SiNmUi+vC/5kyKvHJXyIiIknoLIRs6S6SL0e4Tkwu5eK5HmFkyZdDotiyuadIZyFMtCCwEqAE9JUiOvtC0q7LmLYMvmsIIttfHCokX4yGu4kiIiI1USgH9BQDUi40N6QIopgwBuMYRuVSdPWV6CkEFMpBou1SAlRjvaWQZzv6mDgqi+cY8sWQ0Fo8x3BoewNBZFnX0UdvKUy0AqaIiEgSekoRsa3siuAYQ+MOhX+D2NKY9iiHlp5Ssp0BSoBqrLsQ0FkoM6ElQ8Z3K3ugxOA6lT1QCuWIzT3JlwAXERFJQnPGozHjEcWWhpRLOXx+M9RcyqMnjmnMGJozyaYkqgSdhBeZ12XMi78uIiIyko1uzDClLYfjGIKwMgeoOeuRS7mUwwjHMUwZnWN04673CqsV9QDVWHPWp7XBZ2NXEbd/CGxgD5Rcf0bc1r9dhoiISL0Z25RmzmFt9JYrdfCKQYzFYjA0pFx8z+f4w0YztimdaLuUANVYY9rjkLYG/vrcRowxtDemyPoupSDm2Y4C1lpeMbmVRlWBFhGROuQ4DicfOZatvWXWbstj7cBa6MooyKGjc7zmiDE4TrKDUrrrJiCX9hjXkqYQRJSCiFIUQQxNWZes75JLawm8iIjUr8mjGnjrcYfw4NNbWb2lj2IQk/EdDhub41XTRjN5VEPibVICVGP5ckQ5iJkzdTTb8iWe216gFMakPYdD2rK0NaQplWPy5Ui9QCIiUrcmj2pg0v85hC29ZYpBRMZ3GdOYSrznZ4DuuDUWRjFhbMn2FzpMew7GGFKuwVrwXYdCEBFG8TC3VEREpLYcx2Fcc7KTnXdHCVCNea5DMYxY29FHFMe0NKTwXYcgiunIl+kqhIxtTuO5WpAnIiKSFN11a6zBdyiHEVt6SrQ3Zkh7Lo4xpD2X9sYMW3pKBFFMg6+fQkRE6pu1lt5SSGdfmd5SiLXDVwdGPUA11hfElS0wmlJs6SnRnPXwHYcgjukuhIxpSpFyHPqCmMa0kiAREalPXYWAZ7bl6egtE8aVHRHaGlNMGZ2jZRhKwSgBqrEwikn7LkeOb2ZjV2GnSdBjm7IUNQdIRETqWFch4K/PddFbDMikXDKuQ2QtGzoL9BRDXj6pJfEkSAlQjXmug+cYOvOV+gfPbC9QDmJSvkMUV3qHcmlPc4BERKQuWWt5ZlueLT0lYmvZ0Fms7IlpDM0NPoUgZm1HnpdPbMEYk1i7lADVWC7lEsaWe/62kXwpJO05eK6hVI74y3PdPL21j3+ZNYlcSrWARESk/uTLlYVAHfkyURzTlPEHFwNt6y3hOg7eNsO09sZEy8Go26HGrLWs2VrJfI2BbMojl/bIpjyMgS09JdZsyw/rRDAREZFaCcKIDV0Fwija5WKgIIrY2FUgCJPdDV4JUI1t7inx9JYeJrVmGNecoRxFdBZCylHE+OYMk1ozrN7Sw+ae0nA3VUREZL8rR5ZCKSLj77p3J+t79JUiylGyHQFKgGpse75MdzGktcGv9PLEYKj8GVtLS4NPVyFke7483E0VERHZ71KuoSHlUQh23cNTCCIa0h4pN7n5P6A5QDXnOobQWjZ2F3GMwfcdUgashd5SSHcxILKVOBERkXrjey4TWjJs6imypadY2RHBMdjYUgpjfNcwrimD7yU7F1YJUI2Na06TcR3WdvQxtilDT7GMtZUdcDOey+aeIlNGNzCuOT3cTRUREdnvcimXyaMb6CoGbCkUeXJzkSCK8F2Xya0Z2pszHNrekPhiIA2B1ZjjOIxvzhDGlue6+rDW4rsGayvPw9gyrikzbJvBiYiI1JIxhlG5SjHgtR19lQVBnosx8ExHH1t6SrQ2pBJdAg/qAaq5MIppzHpMaW+gMx9SCCL6ggiDoTWbprXBozHrqxCiiIjUJWst6zoKFIOY5mwKLBgHbAykoRjErOsocEhrVnWA6kk5ssQxHDOhlb5ywPquEsUwIuO5TGxJ05DyKYbJz34XERFJQm8p5IkNXTSmXWaMa6QURkQxuA6kPZcNnQVWbezi5ZOaacokVw162MddbrzxRqZOnUomk+H444/nwQcf3G3snXfeyezZs2ltbSWXyzFr1ixuvfXWITG9vb184AMf4JBDDiGbzXL00UezbNmyWn+N3Uq5hmzapRiGlYk/wGB+awyFMKQh7SY++11ERCQJ3YWAzr6AUbkUBkPG88ilPDKeh6EyPLY9H9BdCBJt17D2AN1+++0sXryYZcuWcfzxx7N06VLmz5/PqlWrGDt27E7xbW1tXHbZZcycOZNUKsVPf/pTFi1axNixY5k/fz4Aixcv5pe//CW33XYbU6dO5Wc/+xkXXnghEydO5PTTT0/6K+J7Lq3ZFH97rot12wtDXnt6ay+HjMry6sPHJD77XUREJDH9/41vbWXlVxRbXMeQ9hysZYeegeQMaw/Q9ddfz/ve9z4WLVo02FPT0NDAzTffvMv4uXPncsYZZ3DUUUcxffp0PvShD/GKV7yCBx54YDDmd7/7HQsXLmTu3LlMnTqV888/n2OPPfZFe5ZqqcF36Owrs66jUFn7PjDVJwb6x0W7igEN/rB3xomIiOx3zVmf1myKDZ0F1m3v4+ktvaze2svTW3pZt72PjV0FRmVTNCe8Geqw3XXL5TIPPfQQ8+bNe74xjsO8efNYsWLFHt9vrWX58uWsWrWKk08+efD4iSeeyI9//GOee+45rLXcd999/P3vf+ef/umfdnuuUqlEd3f3kMf+0lsKWduRJ7aWMI7pLgWVrr5SQBjHxNaydlue3lK43z5TRETkQNGY9jikLcv6ziJrt/XhOIbGtIfjGNZu62N9Z5FDRmcT3QcMhnEIbOvWrURRxLhx44YcHzduHE888cRu39fV1cWkSZMolUq4rsuXvvQl3vCGNwy+fsMNN3D++edzyCGH4HkejuPw1a9+dUiS9EJLlizh6quvfulfahc2dpdY31Ukii1dhRAbW6yBMIRyFNOS8Xmus8DG7lJldryIiEidaUz7jG3OUAhCekshPQWLcQxNWY+M75FLJdv7AyNwFVhTUxMrV66kt7eX5cuXs3jxYg477DDmzp0LVBKg3//+9/z4xz9mypQp/PrXv+b9738/EydOHNLbtKNPfOITLF68ePB5d3c3kydP3i/tDcKQTV1F8qUQ3zVEGCwWYwyuA13FgMhaglA9QCIiUn/y5YhSEPHyic08ubmH5zqLBFGM7zqMbcpw+JgmSkFEvhwl2gs0bAlQe3s7ruuyadOmIcc3bdrE+PHjd/s+x3E4/PDDAZg1axaPP/44S5YsYe7cuRQKBT75yU/ygx/8gNNOOw2AV7ziFaxcuZLPf/7zu02A0uk06XRtKjFHsaW3FNJXrix9h8pcL4slDKEYRpj+OBERkXoTRjHdxYDOQkBDyuPlk5oxplIQuBTEbM2XiaxNvB7esM0BSqVSHHfccSxfvnzwWBzHLF++nBNOOKHq88RxTKlU2Uk9CAKCINipqrLrusTx8BQadAAslIOIMI4xVApAGSCMY8pBhOUAqEcgIiJSA65j2NZbpqcQ0N6UJuU5OMaQ8hzam9L0FgM6esuJ74k5rENgixcvZuHChcyePZs5c+awdOlS8vk8ixYtAuDcc89l0qRJLFmyBKjM1Zk9ezbTp0+nVCpx9913c+utt/LlL38ZgObmZk455RQuueQSstksU6ZM4Ve/+hXf+MY3uP7664flOwYWfM8h5RnKYUyZHXp6jCXlGVKuQ6AOIBERqVuV5e9rt+XpzAeENsYzDq05n9hCLp38TXBYE6AFCxawZcsWrrjiCjZu3MisWbO45557BidGr127dkhvTj6f58ILL2TdunVks1lmzpzJbbfdxoIFCwZjvvOd7/CJT3yCs88+m46ODqZMmcK1117LBRdckPj3A2jwHDK+S8l3wRqCOGag5IHvuGAsGd+lwVMfkIiI1J8otmRSLls29bKpu0Tad3AdiGJYu73A2OY0E1oziU8FMdZa9T28QHd3Ny0tLXR1ddHc3PySzvXstl4uvfMvPNdZIOu7uP3jnsYYImspBBGTWrN89q3HMHl04376BiIiIgeGnmLAbSvWsGpTD3FssTvsBWYMOI7hyPFNvOv/Tn3JW2Hszf17xK0CG2l8z2VCc4ZSOaKrHJIvhsRU5vxk0i7tDSkmtmRUCVpEROqStZaOfBmAae05uvrCwVVgLQ0ez3T0sb03IOn+GCVANZb2HMa1ZNnYXSSOYzKuO9gDlDKVbsFxLQ2kNQQmIiJ1qKcY4joG33X509oOiuVKEWDHGDIph4nNORynEpdkPTwlQDXmuQ65jEdDyqWnGFAoh0TW4hqDk3ZpSLnk0i6eqwRIRETqUzGI2NzdR0dPiDW2vxwM9JViPNNHLp38FBAlQAkolkLy5Rjfc2lL+TjExDjEcUy+HFMsqwiiiIjUp8a0y+aeEhu7yoxtTuM5DsZUtscM45iNXSUa02Ua08lOBVECVGNBGNHRV8YYGN+cxmCwxmCsxWLpLIRs6ysThBGQfClwERGRWuorRxSCCN8zlV0QXIPbvxAosgbfMxSDSsHglobk2qUEqMY6CyG9pYgJLVm6C2U6+kpEEbgujG5IM6ElS28xorMQ0qZFYCIiUmc6CwFp1+GQUVnypYj8Dpt/e65hUmsW0x83oTW5dikBqjHHVLa92Npb2fvEWHAci7GG3nJAMYpoyfokXABTREQkEa7jkPZdGnwXKNORD4hii+sYWrIerdkUhSDCdZKdC6sEqMYyvouNLZt7SsSxxTUOGAvW0FMKcRxDc8Yj42sZvIiI1J/xzWlasj7/2NzLqIYU45rSDEwCii2s7ehjxrhGxjfXZk/O3VECVGNZ3yGKoVAO8RyHfBA8v/zPdyiFEXFciRMREak3jWmPQ9tyPLa+m01dhYGhkcqWCLEFY5jclkt0J3hQAlRzPcWIKI4Jw5jeMCLun/xsMBQDQ8YzhFFMTzHZyV8iIiJJ6AtiWrIeo3I+f9/YQz4MsZHFuIac53HE+CZaMh59QUxjOrnOACVANdZXCtnWFxBaSzmMCGK7w15gBs912dYX0FfSUngREak/QRixpiPP9nwZjMHHwbqV+6Axho58mWc68pXV0An2AikBqrHYxmzpKdJbjPAcM7Tis7X0FiO29hSJbTx8jRQREamRYhDxl3WdbO0t05ByaUq5gx0BEbCtt8yj67ooBlGi7VICVGN95ZBCOSCMI4zj4tqBn90SAWEc0VcO6FMxRBERqUOdfWU2dJUIoggvMvQE8eBUkIzvEEQRG7uLdPaVmdCa3FwQJUA11lsMCePKpGcwWDOQ/lQmgTnGEMaW3qISIBERqT/b8mXKQWUKSFAISXkuvmMIY+guhGAspXLEtv4NU5OiBKjGLJUkJ+U5eK5DHD+/263vGkJTSY6S3QNXREQkGSnXUI4tcWzJ+C7FMMJWFn+R8RyKQUhgLSk32YJ4WntdY80Zn1zKr+x9Yi22f/WfNWCsxXMccmmf5oy2wRARkfqT8V1SriGILIVyCFgcY4HK8yCy+I5JvB6eEqAaa2/OML4lAxYia/Edh5Rj8B2HyFqwML45Q3tzZribKiIist81pj0a0x7WVsr+OBhcDA6G2FY2RR2ISZKGwGqsJeszZXSOzT2lymTo6PnJX75ryKZ8prbnaMmqB0hEROqPxdCY8mht8ClHMbG1RLHFOIa075ByHRpTXmVubIKUANWYMYZxTWnGNKbpLkIhsMRRjOM6ZH1DcybNuKY0xmgzMBERqT8p16GlIUUYx5RCS2chIDQWzzGMavBJuYaWXIqUq73A6koYxaR8h7ZcikIpYHtQIgwtnhfTkk4zqiGF77mEkeoAiYhI/XEcw+imNB19ZawNGN+cxXFi4tihUC5jfJ/RjWmchHcFVwJUY+XIsj0fsG57L890FCiFcaUAVAn6ShExlkmjspQjrQMTEZH605TxaEy5NGUqy9+39wVEcYzrOIxqSJFJOTSlXZoymgNUVzxj+etznTy5KV+Z/OUwOMpZDGKe3JQnl0rhGSVAIiJSf4wxNKQ84tiQ8Rwmj8pgMFgsYWSJY0PW9xKfCqJVYDW2rbfIU5t7CaKB5McZfDgOBBH8Y3MP23qLw91UERGR/S6MYnzP0JzxCbB0FUO2FwO6iiEh0JT28T0n8akg6gGqscfXd9NTCqkktg7xDiUPDQ7GxPSUQh5f380RE0YNWztFRERqoRxZ+koRjRmXOPYpuTGRBddA2nVoyrr0lcLEp4IoAaqxDb0BYQyOBYPF7S+EaIDYWoyFMK7EiYiI1BvfgWIY01uMaM759BQiwriyCqwp69JbiGhIx/gJj0kpAaqx1rSLqdQ7BCDqL/pkBvcEA2MrcSIiIvUmiAEsxTCitzPA8xxMDCUL+c4Az3Ww1vbHJUcJUI0dPjZH1oe+AKL+jHdgFnQYWyzQ4FfiRERE6o3vANZQDCO295bYlg8Ge4BG53xGNaYrxYET7gHSJOgaG9vcwPhRDbgOxECEJcb2/wmuAxNGNTC2uWG4myoiIrLfBTH0lkPWbsuzvqtIEMc4WII4Zn1XkbXb8vSUQvUA1ZuU73LUhBbCKGJbb0ApjImpdAI1+A6jG32OmtBCKuFN4ERERJLgErN6ay89hYBMysMxBodKp0BsLT2FgGe29uKiVWB1Je05HDG2iXwxxDU9bO8LiWKL6xhGNXhMGd3EjLFNpD11xomISP1Z31Vka0+JtO/SmPbZcceLKAZrLZt7SqzvKtKe4GiIEqAa8z2XpgaPMLakXJexTR7WWowxOFjC2NLU4OF76gESEZH6sy1fJogsLVkf3zWUQktsLY4xpD0Hk/XJlyK25cuJtksJUI1lPUNnvkyhHOE4Dh2FgDiMcTyHtgafQjmiuy8g62kzVBERqT+e61YSHfN88mMtxFhKYYzjVBIhz022I0DjLjW2NR+wbnuBjnyZrmKJ1ozH2JYUrRmPrkKJjnyZtR19bM2rDpCIiNSf6e0NtOV8egohMZX6dwb6S8RYegshbbkU09uTXQykBKjG8sUya7blcQ00Z1L0FEO29ob0FEOaMymMgTXb8uSLyXb9iYiIJGFcc4YjxjUTW0uhGFCKIsIoohRF9BUDYms5ckIT45ozibZLQ2A1tjVfpjMfEEYx24shQf8qMAcoRRE536MzH7A1X+aw4W6siIjIflYILUeOa+LxDT08uz1PGASVzcENeK7D5FENHDGmiUJoaUxwFEwJUI2lHEMxiNjeV8L3XHzPxTEQWwiimC2lIqNyaVKO5gCJiEj9CcKI7lLAuOY0kY3pygeE1uIZQ2vOZ2xTmu5SSBBGkE4uLVECVGPGQBBFxNYSR5YgjioHrSW2lRoIQRT1b5YqIiJSX0phzJoteYphxOTWBia22sFyMC6GfBCyZmsvpVB1gOpKFFsMBpdKrYNSDNgYjMFzwAWMNURxsrvgioiIJCGIYrqKAaUgIu25FINocE/MjO9SCiK6iiFBlGwCpEnQNRbEFs9zwDWE1lZmvvdvBxbGFusafM8hUAIkIiJ1qFCuJDxhDNvyJcxA/R9j2JYvEcZgY0uhHCXaLvUA1VjWd3GMweAQxxHlaCDRsaTcSn0E4xiy2gpDRETqUNZ3cIzBdw1NmRTFIKIUVToERjem6ekLcBxDNuHdUJUA1VhLxsMYS1+pktnuuONFGENYimhrtLRk9FOIiEj98T2X5qxHEMf4rsF3vMowiAUMpHyHlkzyOyLorltjhSAmCMHaym9t7fO/e/9vTxhW4kREROpN2nOY0p6jHFg6+kqUw3jw/pf2XFqzKaa05xLfE1MJUI1t7S3RVw7wXIjjyvL3gUEw14DjQL4UsLW3NKztFBERqQXfcxnfnGXttgJ+ySWb9QYWQxNFlkzKZVxzNvEeoH1KtzZt2sQ555zDxIkT8TwP13WHPOR5XcWgku1acN1KiYOBh+tW/gGUw8oMeRERkXrT4Du4DjRlXF5xSDONaQ/XGBrTHscc0kxTxsVzDQ0jYQ7Qeeedx9q1a7n88suZMGECRkVsdss1/QNe/dlusMNiL8dQ6QPE9seJiIjUl74gJu26eMbw0DPb6egLiKIY13XY0lPk6AnNpByHviCmMZ1cErRPCdADDzzAb37zG2bNmrWfm1N/mjIeruNQDOLK8nfz/NyfKK4kRRnPpUmToEVEpA6FUczmniJ/29jNxs4CrufiOIYwtmzoLGANTGjNEo6EOkCTJ0/GWtWtqUZzxifjO4O1f1wDnqn8OVATKOMbmjP+cDdVRERkvzNY/rimg229Zca1ZBid82nN+ozO+YxrybCtt8xDz3RgSDav2KcEaOnSpVx66aWsWbNmPzen/lggk/LIeAbXgchCaCt/ug5kPEM25SX8s4uIiCRjS0+JTV1FbAzdfQHrthdY21Fg3fYC3X0BNoYNXUW29CS7GGifxl0WLFhAX18f06dPp6GhAd8f2nvR0dGxXxpXD1xjaEh5FFIBpTAmiOzgbvC+Z0i5DtmUpzlAIiJSlzryZfqCmJ5ime5iBMSDW2Fszwc0Z1wcJ01Hvpxou/YpAVq6dOl+bkb98l2HrO8Qx4YgsoQxYCE2QGjxTaX6pe9qVxIREak/ngM9hRJdxRDHGFzHxQFiKvtlVo4PLRScSLv25U0LFy7c3+2oW40Zjyi2FKOYqL8OEAC2Mgm6ctzSqEnQIiJSh3Iph0IQE0YxY5oyQwoBG2BLT5FiGJNLjYBl8ABRFPHDH/6Qxx9/HICXvexlnH766aoD9AJBGNFdDCkGMQ6VDHegAFQcQzGI6SmFBGGym8CJiIgkYVNPgO86pD2XfDkk7Tq4jiGKLaUoJu25eI7Dpp6AIxNs1z4lQP/4xz849dRTee655zjyyEpzlyxZwuTJk7nrrruYPn36fm3kSLaus0B3oYzXv/w9tlT6/fqXxHtAV6HMus4CU8Y0D29jRURE9rPYQjbt0pRx6ciX6CtFg3NhU55hdHOayO4wQpKQfepvuuiii5g+fTrPPvssDz/8MA8//DBr165l2rRpXHTRRfu7jSPa9ny50vvT3/MTx5UVYHFceW4cKJVjtic8+UtERCQJh4zKMiqTohRYXONQjqEcWcoxOMahHFhGZVMcMiqbaLv2KQH61a9+xXXXXUdbW9vgsdGjR/PZz36WX/3qV3t1rhtvvJGpU6eSyWQ4/vjjefDBB3cbe+eddzJ79mxaW1vJ5XLMmjWLW2+9dae4xx9/nNNPP52WlhZyuRyvetWrWLt27V61a3+JLFhrKYcQ9I9yDaz3CiIohxBbS6R18CIiUoemtecY35pmc2+RzmKI60DKqZSG6SyGbO4tMqElw7T2XKLt2qcEKJ1O09PTs9Px3t5eUqlU1ee5/fbbWbx4MVdeeSUPP/wwxx57LPPnz2fz5s27jG9ra+Oyyy5jxYoVPProoyxatIhFixZx7733DsY89dRTnHTSScycOZP777+fRx99lMsvv5xMJrP3X3Q/GJvziS1UFv5V/hx4xP0PaytxIiIi9cZaSym0OMZUVn/tMBLiAI4xlKM48QLL+5QAvelNb+L888/nD3/4A9ZarLX8/ve/54ILLuD000+v+jzXX38973vf+1i0aBFHH300y5Yto6GhgZtvvnmX8XPnzuWMM87gqKOOYvr06XzoQx/iFa94BQ888MBgzGWXXcapp57Kddddx//5P/+H6dOnc/rppzN27Nh9+aovWSHcYfv33bC2P05ERKTOPL6xhw2dBVpzKRpSLo4DYHEcaEi5tOZSPLe9wOMbd+5YqaV9SoC++MUvMn36dE444QQymQyZTIZXv/rVHH744XzhC1+o6hzlcpmHHnqIefPmPd8Yx2HevHmsWLFij++31rJ8+XJWrVrFySefDEAcx9x1110cccQRzJ8/n7Fjx3L88cfzwx/+8EXPVSqV6O7uHvLYX8pBQHkP25uU40qciIhIvdncVaSzL8A30N6UYVxzlnEtWcY1Z2lvyuAb6CwEbO4qJtqufVoF1trayo9+9COefPJJnnjiCQCOOuooDj/88KrPsXXrVqIoYty4cUOOjxs3bvCcu9LV1cWkSZMolUq4rsuXvvQl3vCGNwCwefNment7+exnP8unP/1pPve5z3HPPffw1re+lfvuu49TTjlll+dcsmQJV199ddVt3xubugt73ObC9seJiIjUGwMEcQyOgx9X5r1iIcZWigRbCKKYpPdDeEnV92bMmMGMGTP2V1uq0tTUxMqVK+nt7WX58uUsXryYww47jLlz5xLHla6Wf/mXf+EjH/kIALNmzeJ3v/sdy5Yt220C9IlPfILFixcPPu/u7mby5Mn7pb1besP9GiciIjKSjG/NkHYNPYUQr2Gg/CFgoUxEvhDSlHUZ35rsXN2qE6DFixdzzTXXkMvlhiQLu3L99dfv8Xzt7e24rsumTZuGHN+0aRPjx4/f7fscxxnsaZo1axaPP/44S5YsYe7cubS3t+N5HkcfffSQ9xx11FFD5gm9UDqdJp1O77HN+yKucpvTauNERERGkqaMz/iWLN2lHrblyzjYwVLQMZXVYOObszRlkl0MVHUC9MgjjxD0z1N55JFHXvIHp1IpjjvuOJYvX85b3vIWoDKHZ/ny5XzgAx+o+jxxHFMqlQbP+apXvYpVq1YNifn73//OlClTXnKb98XkKjPaauNERERGEt9zGdOU4dmOPN2FkJDnt8JwsDSmPMY0ZfC9ZHeSqDoBuu+++3b595di8eLFLFy4kNmzZzNnzhyWLl1KPp9n0aJFAJx77rlMmjSJJUuWAJW5OrNnz2b69OmUSiXuvvtubr31Vr785S8PnvOSSy5hwYIFnHzyybz2ta/lnnvu4Sc/+Qn333//fmnz3hrfXF1iU22ciIjISOI7EEQRxnFpykC5fw9M1zGkXAfjuIRxjD8SNkN997vfzRe+8AWampqGHM/n83zwgx/c7TL2F1qwYAFbtmzhiiuuYOPGjcyaNYt77rlncGL02rVrcZznr0g+n+fCCy9k3bp1ZLNZZs6cyW233caCBQsGY8444wyWLVvGkiVLuOiiizjyyCP5/ve/z0knnbQvX/Ul6wsg5UL5Rbb6SrmVOBERkXrTWQjpKYY4WPrCmCCulM8JLcQ2ptF16C4EdBZCRjft+Xz7i7H7UHnIdV02bNiwU22drVu3Mn78eMJwZE/o7e7upqWlha6uLpqbX9r+XD9+5Fk++YO/UijH7CoHcoFsyuEzZ7yc0//P/pl4LSIicqD427rtfPi7K9nUVaIcRkNmvBoqQ2QTWtL815mzeNkho17SZ+3N/XuveoC6u7sHCx/29PQMqa4cRRF33333sBUcPFCNa87gGHaZ/EDluGMqcSIiIvWmFEZsz5cphBEOlaTH7vBnsf/1UvgiQyU1sFcJUGtrK8YYjDEcccQRO71ujKlZPZ2RqjXrscdS0Mb2x4mIiNSXfCmkWI6II/A8g+MYrB3YINwShpZCOSJfSnb0aK/uuvfddx/WWl73utfx/e9/f8hmqKlUiilTpjBx4sT93siRrLsYEu+hErSNKnEiIiL1prsYEFMZ7XCMwTMG64CxEBpwjCXuj0vSXiVAA4UEV69ezaGHHooxSddtHHnWbS8Sxi/eAxTElnXbi7wqoTaJiIgkJYwMDoaMb8Aw5J7oOpDxHaytxCVpnxad/fKXv+SOO+7Y6fj3vvc9vv71r7/kRtWTMAoJ7UC9g8qfA4+B56GtxImIiNSbCS1pGjMeGIvBEtmYMIqJbAxYMJbGrMeEltoUJN6dfUqAlixZQnt7+07Hx44dy2c+85mX3Kh6kvEddih6iWMqGa9jnp8Ahu2PExERqTOTR+eY2JohCC29JUsQQhhDENL/3DKxJcPk0blE27VPM2/Xrl3LtGnTdjo+ZcoU1q5d+5IbVU8aUil8F8IQYqhkPC8YEfPdSpyIiEi9GdOYoi2Xwtr+++ALWAujG9OMaUz2PrhP3Q5jx47l0Ucf3en4n//8Z0aPHv2SG1VPMr5L1nNxeD732fHhAFnPJeMnWwJcREQkCVt6y2zLlzGmUvvO2+HhUlkNtrW3xJbecqLt2qcE6B3veAcXXXQR9913H1EUEUURv/zlL/nQhz7EWWedtb/bOKK15Tx838ExlXLgLuD2/yPw+4fCUr5DW07L4EVEpP48u62XDZ0FfNchl3bwfIPnGTzfkEs7+K7Dhs4Cz27rTbRd+3TXveaaa1izZg2vf/3r8bzKKeI45txzz9UcoBcwxiHjeaTcAIsBBywWQ2W2u8GS9j2M0RwgERGpPxu7S/SWIzzXkPFcMhastRhjcEylEGJvOWJjdynRdu1TApRKpbj99tu55ppr+POf/0w2m+WYY44Zth3XD2SFMCblOzSkPQrliLB/5xGLxTeGTMoj7TkUwj0UCxIRERmBHMdAbAmtJXbdwb3AjAHfMYRRDLY/LkEvadzliCOO2GVFaHleyjF4joMBUq6Dix2sgOn19wO5jkMq4R9eREQkCaMaUqR9j55SQFgIsDusBDIYYixNGZ9RDclOgq46AVq8eDHXXHMNuVyOxYsXv2js9ddf/5IbVi/SvouDJbaWiMpyv8EhsEpZBFxjSWsStIiI1KFDRmVpynp09gVEO20NVXnelPE4ZFQ20XZVnQA98sgjBEEw+PfdUXXooVKuwRgoBDHlyNLf04fBEsSWlGsG40REROqN7zpkHAfHYXAExFiw5vnnGdfFd5OdC1t1AnTfffft8u/y4vrKEd3FkFJgh+wIb6kUgrKxpbsY0ldOdhdcERGRJHQXygTW0pT2gf7OAFtZEV35j3+HII7pLpSZOCq5Yohae11jvaWAjnyJ3aU3EdCRL9NbSnYTOBERkSQM1Pdpz3nkyxGOExPbShmYjOuQTbnE/XEzE2xX1QnQW9/61qpPeuedd+5TY+rRs9v66CtXxjgHt77oN/C8UI55dlsfs3curi0iIjKiZX0P33UoRRHGgGMqY2GO6d8cFUi7Dlk/2T6Zqj+tpaVl8O/WWn7wgx/Q0tLC7NmzAXjooYfo7Ozcq0TpYLC+uzBY+nvXU78qvUDruwvJNUpERCQhE1szZHyHzT1F7A6lYExkCCIwQcyUNp+JrZlE21V1AvS1r31t8O8f//jHOfPMM1m2bBmuW1m9FEURF154Ic3Nzfu/lSOYV+Xy9mrjRERERpJc2iOX9igHETHgOA4uhpBKEWUHyGUqMUnapynXN998MxdffPFg8gPgui6LFy/m5ptv3m+Nqwdjq9zcrdo4ERGRkaS7EBBFMZmUh++YSj08LC4W360UBA4jS3ch2bmw+5QAhWHIE088sdPxJ554gjhWReMdpd3q6vtUGyciIjKSbM2X6S1HjM6laMy4xMYQxxAbQ1PaY3QuRW8pZGs+2c1Q96m/adGiRbznPe/hqaeeYs6cOQD84Q9/4LOf/SyLFi3arw0c6fJBhAO8WFro9MeJiIjUmyiKKQUx+SAijg1ZzyF2K5Ogwxh6yhE5W4lL0j4lQJ///OcZP348//mf/8mGDRsAmDBhApdccgkf/ehH92sDR7rYgudUav7s6qd1qLwev3CGtIiISB3IpT2CKKanr4zrGIauiY6JSiHppnTic4D26dMcx+FjH/sYH/vYx+ju7gbQ5OfdGNOUwXMM5d1kODGVCdBjmpKd/S4iIpIEzzFYLKGNAZfKhI/Kwp8ICG2MxSa+GGif606HYcgvfvELvv3tbw9uf7F+/Xp6e3v3W+PqwaTWNO4eynu7nsOk1nRCLRIREUlOKYgxprIxuGsMoa0kQ6G1uP3HwVAKRsAQ2DPPPMM///M/s3btWkqlEm94wxtoamric5/7HKVSiWXLlu3vdo5YhSDG2KG9PzsVRLSWQsI/vIiISBJ6SiHGQDblki9FRHFlg3DHGKw15NIuxlTikrRPPUAf+tCHmD17Ntu3byebfX731jPOOIPly5fvt8bVgy09ZSzgA54Bl8pFd/uf+1Q2g9vSk+zsdxERkSQ0pV1sDIVSjO8YPNch5Tp4roPvGAqlGGsrcUnapx6g3/zmN/zud78jlRpau2bq1Kk899xz+6Vh9aK7FGKBhoxDqRwT2Mq8H4dKApROO0RxJU5ERKTepH0Xay1RHGMxDM4KMbayQCi2WGtJ+8kmQPvUAxTHMVG087LtdevW0dTU9JIbVU8mNKXxXUNfOcYCrvP8wwJ95RjfNUxo0hwgERGpP2EUY0xlMrTrGGIcMA4xDq5j8ByDYypxSdqnBOif/umfWLp06eBzYwy9vb1ceeWVnHrqqfurbXVhTHOGtFfp5SnHEMVg+/8ceJ7xHMY0axWYiIjUn3w5wnddGtIeBouJI6IowsQRBktD2sNzXPLlZOvh7XMdoH/+53/m6KOPplgs8s53vpMnn3yS9vZ2vv3tb+/vNo5oWd/BdQ0DlQ+g0vNjeD77dF1D1t/nBXkiIiIHLNep9PQ4WOLYUoos1oIxkAYcLK5jcJ1k74P7lABNnjyZP//5z9x+++38+c9/pre3l/e85z2cffbZQyZFC2zoKmGtIeMbov5aQDYG0/87u44htoYNXSUOGa3hQxERqS+jcz7GWLYXAgyGlGOwDpi4UiR4eyGgJZdidM5PtF17nQAFQcDMmTP56U9/ytlnn83ZZ59di3bVjc5CSGyhMe3RF0SEkQXXYjD4riHru8S2EiciIlJvXMcQRJbYVv5uDYDBmkqnQBxbgsj2V4lOzl4nQL7vUywWa9GWutSScSG2lMOYloxHKap0ATqOIe0a8qWItG8qcSIiInVmU0+JUhCT8SoFDy2Vys/GVIbHfMehHMRs6ikxqa0xsXbt04Db+9//fj73uc8Rhuq12JNRuRTZtEs5jtmaD+gqBPQUQ7oKAVvzAeU4piHlMiqX2vPJRERERpiuvoAYSy7tkUk5uK6Ha1xc1yPjOzSkPaL+uCTt0xygP/7xjyxfvpyf/exnHHPMMeRyuSGv33nnnfulcfUg47vk0i7lDkton58IDVT2PjGQS7tkEq5/ICIikoS05+AAAVRWAcWVsjAmtuA4WFOZCJ32RsAk6NbWVv71X/91f7elLsXWUig9v7RvxyoHA8lQoRwTW20HLyIi9WfSqCzplMv2zhKOU1kBZi3EBqIwIo4NE1rTTBqV7CKqvUqA4jjmP/7jP/j73/9OuVzmda97HVdddZVWfr2IDV1FthcCsEP3/4L+5xa2F8ps6CoypV2rwEREpL6kPJfmlM+6uA8bgeMYHCCylQnQxkBT2iflHcCVoK+99lo++clP0tjYyKRJk/jiF7/I+9///lq1rS5095UolEJ2N1sqBPqKId19pSSbJSIikoggiomxZDyXtO9ggchaLJD2HTKeS2wtwYFcCfob3/gGX/rSl7j33nv54Q9/yE9+8hO++c1vEsfayXx3OvtCStHOvT8DLFCKKnEiIiL1ZltviWIY05D2KkNfcWVJfGUPMGhIexTDmG29yXYE7FUCtHbt2iFbXcybNw9jDOvXr9/vDasXxsTsKT2M++NERETqTRRbCuWInkKZOIa051Z6gzyXOIaeQplCEA0WC07KXiVAYRiSyQzds8r3fYIg2aVrI8mOE6D3R5yIiMhI0uA79JVDgtD2735giWJLbCvPg9BSKIU0JLwl1F5NgrbWct5555FOP79zebFY5IILLhiyFF7L4J9XCKvr2ak2TkREZCTpLkXY/jk/QRThmMo8IGMhiKPKynhr6U64I2CvEqCFCxfudOxd73rXfmtMPUp5Lg7PL383PL8Z6kBnn9MfJyIiUm+CMMJYg+NCGFVWlA/cB12n8jDWEIQHcAL0ta99rVbtqFuT27KkPXixrb4yXiVORESk3sQYYgPEEMeAqfT+YCCKwTWVmkAxB/heYLJ3Zo5vpi2XYlN3GagUfxrIfE3/b93WmGLm+OZha6OIiEitTGjysTYmiKDBN1jzfKJjrKUQVCojTmhKdjf4ZGccHYQa0j5HT2gh4zuDF3vgp3eAjO9w9IQWGtLJ/vAiIiJJ2F6M8R0H3zOE1uIZi+eCZyyhtfiewXMM24sHcB0g2XtBDIe25xjTmCKwEPH8I7AwpjHF5NE5As2BFhGROmTjmGzaZ1TWI+O7FCMolmOKUWW/zFFZj2zaxyZcU1BDYDXmO7B2Wy9bekq7zDa39JR4tqOXhFf/iYiIJCKT8mhMO3RFLhkg67uDc0GstbiuS2PaIZNKNiXRbbfG+koBjz/XTTHs7/Jzwe3/03OhGFoeW9dDX0m1lEREpP5MHd3A6Fwaa2Nc16EQxPQFMYUgxnVdrI1pb0wzdXRDou1SAlRjj6/vpqPv+QnQsa3Meo/7d8MF6Ogr8fj67mFspYiISG24rsvEUQ2EMRTKMY0Zj5YGj8aMR6EcEcYwobUB1022HIyGwGpsfWeBIKz09gU7VPkeqPjtAEFYiRMREak3QRjhGhjTlKarEFAIYiwWgyGX9mjJeriG/jpAyS0IUgJUY8YxxFQmPe9KtEOciIhIvekshHT0BYxpztCS9egtxURxjOs4NKYdUr5HR19AZyGkrTG5dikBqrFJLekhVaB3THNs/yPujxMREak3Bks5CAmCmFzGBxMTW4tjDLm0Q74Y4mAxHMCbodbKjTfeyNSpU8lkMhx//PE8+OCDu4298847mT17Nq2treRyOWbNmsWtt9662/gLLrgAYwxLly6tQcv3LLLg9Wc9A8nOwGPgp/ZMJU5ERKTeWAzGGApBzLbeMg6WtGdwsGzrLVMMYowx2IQrQQ97AnT77bezePFirrzySh5++GGOPfZY5s+fz+bNm3cZ39bWxmWXXcaKFSt49NFHWbRoEYsWLeLee+/dKfYHP/gBv//975k4cWKtv8ZuBTGkfYfdTe1yqbyuOkAiIlKPWjIuWd8lspWCiB19AZu7i3T0BfiuQ2hjGlIuLZlkJ0EPewJ0/fXX8773vY9FixZx9NFHs2zZMhoaGrj55pt3GT937lzOOOMMjjrqKKZPn86HPvQhXvGKV/DAAw8MiXvuuef44Ac/yDe/+U18f/iqLDdmfFKeg+ewUxLkAp4Dac+hMaNK0CIiUn9Ca2jO+jg4dBfL5FIuLVmfXMqlu1DGxaEp4xPag6gHqFwu89BDDzFv3rzBY47jMG/ePFasWLHH91trWb58OatWreLkk08ePB7HMeeccw6XXHIJL3vZy/Z4nlKpRHd395DH/jKhOU3GdwnjnSdCR0AYQ9p3mdCsOUAiIlJ/fAdSrktrg09TOkVXIWBLT5muQkBzJkVrg0/acxMvCDysk6C3bt1KFEWMGzduyPFx48bxxBNP7PZ9XV1dTJo0iVKphOu6fOlLX+INb3jD4Ouf+9zn8DyPiy66qKp2LFmyhKuvvnrfvsQeZHwX1zEvugrMcwwZP9muPxERkSQEMRhjcRxIezC+JYOxFmsMNo5xHACb+FSQEbkKrKmpiZUrV9Lb28vy5ctZvHgxhx12GHPnzuWhhx7iC1/4Ag8//DDGVNed9olPfILFixcPPu/u7mby5Mn7pa09xYDeYjC4AmzH39ehMhG6txjQU1QlaBERqT++07/qObaUI8v2nuLgMvhRDT6eawfjkjSsCVB7ezuu67Jp06Yhxzdt2sT48eN3+z7HcTj88MMBmDVrFo8//jhLlixh7ty5/OY3v2Hz5s0ceuihg/FRFPHRj36UpUuXsmbNmp3Ol06nSadrMwS1emueUhjjA7F5vvozgGPAsVAMY1ZvzfOyQ9pq0gYREZHhEsRQimI6CiFBGDE6l8Z3IYigu1imFFnGR3HiPUDDOgcolUpx3HHHsXz58sFjcRyzfPlyTjjhhKrPE8cxpVIJgHPOOYdHH32UlStXDj4mTpzIJZdcssuVYrXWU4qIY1sZAjOVSc8DD0xlCCyOLT2l3Q2SiYiIjFyesfT0hTjApFEN+J7BGoPvGSa1NeAAvYUQzyRbD2bYh8AWL17MwoULmT17NnPmzGHp0qXk83kWLVoEwLnnnsukSZNYsmQJUJmvM3v2bKZPn06pVOLuu+/m1ltv5ctf/jIAo0ePZvTo0UM+w/d9xo8fz5FHHpnslwOaUu5g3R8PhpR5MvQnQKYSJyIiUm+6ihGhtbQ3pfAcQ2PWq9wUHSCG9qYUQWzpKkaMbkquXcOeAC1YsIAtW7ZwxRVXsHHjRmbNmsU999wzODF67dq1OM7zHVX5fJ4LL7yQdevWkc1mmTlzJrfddhsLFiwYrq/wosY1p/AMlHl+/y8sYJ6vBO31x4mIiNQbx0Au5RHHlt5imU09ZYIownddxjWlaEyncDxD0jtCGWutahC/QHd3Ny0tLXR1ddHc3PySzvXH1Vu54NaH2NYXAjtvhQEwusFj2TnH8app7S/ps0RERA40m7qLfOVXT/G39V1s7C4ShBZrY4xx8D3D+OYML5vYwvmnTGdcc+Ylfdbe3L+HvRBivXMN+K5D2jy/6mvg4QBpAynPwdVeqCIiUofacz6lKOSZbXmCMCblGrIpn5RrCMKYZ7blKUUh7blkCwIrAUqAMRbcSuXnlAMpU/nTBXBJfAM4ERGRpPSWQjZ1lTAYfNdgLURxhLXguwaDYUt3md5SmGi7lADVmO8YDJUiCMaBKK5sfBrFYFz6x8Ec/KQHP0VERBKwemsfHb1lRjf5xNZQCEP6ShGFMCS2htFNPlt7Sqze2pdou4Z9EnS96wstkbUYW5n07jqDc6CxtpKBRtbSF6oXSERE6k8xCOkpB3jGoS3nAzsu+rGUwphSGFAMku0BUgJUY75TSYBiW5kJH+5Q6MlzKivDImvxHSVAIiJSfxpSLlEEIREt2RSRtYMdAa4xdBcLGAwNCZeDUQJUYz2lmNhabH/ys+NIVxBXeoRia+kpJVwCU0REJAGtDSlaG1Js6iqwLV8ijOLBMkCe61AOYsa3ZmltSLYcjBKgGmtMGaIoJo4r+5zEPD8E5vcnRVEc05jSHCAREak/ruMwbUwDW3oKbOwu4mD654FAjKU16zG1vQHXSXZashKgGuspxVg70NPD4KaosMOcoBj1AImISF1qynh4joPnuuR8SzmqjIw4jiHruXiui+c4NGWSTUm0CqzGfMfguWYw6QktBLby5wDPNVoFJiIidclaS3chwGBpynikfAffcUj5Dk1pD4OlpxCQdF1m9QDVmDEOxhiiGHac326p9Ah5FowxGKNcVERE6s+m7hL5UkgYWbYVA+K4vxxwaCiUY1oyXqVWUHeJloZ0Yu1SAlRjrVmXOLbsbnFfSGU3+NasNkMVEZH6E4QRm3tK9JZCojgijulfGV1ZDdZbgs09ZYIwSrRdSoBqrC+ICKJKt56hUv15YBJ01P/3ILL0Bcn+8CIiIkkI45iO3jJ9pRDHqcyEdUzlThjHlr4wZHtviTBOdi6sxl1qbN32AmEUM9C/E1JJfAZ6hFwgjGLWbS8MS/tERERqqRRGFMOQ0FZ6flwHXMcMLg4KLRTCkJJ6gOpLEMbE/fO6Xji9a+B5bCtxIiIi9WZ7PqgkPlQmRJej5++Gpn9HhNhW4pKkBKjGWhr8ysZvu3k9ovKPoqUh2V1wRUREkmBNZSW00z8HJNjhv/d9p9IjZPrjkqQhsBoblfUGe4B2J44rcSIiIvWmLeuRdh3CqLIZ+EA9PEPleRhB2nVoS/g+qASoxrb0lvYYY6uMExERGWlG5dJkUh4WiPpXgA08orhyD8ymPEblklsCDxoCq7mNXSX2NLsn7o8TERGpNynPIeU5GAvGDJ0PawAspPxKTJLUA1Rjjtl58vMLWYZukioiIlIvOvoCwJBJmR1WgD3/yKQM1pr+uOSoB6jWqi3tnXAJcBERkSSEYUQ5isj4Pg0pCOKYOLI4rsF3HCILQRQRahm8iIiI1AvXMbjG4BpoyvqAGSwITP8+YE5/b1CSNARWY1GVHTvVxomIiIwkubRPc8YjlXIY2Bp8INUxGFIph+a0Ry6dbDkYJUA15pvqMptq40REREaSXNrn0PYcLWkPYwYqQffX/zHQkvY4tD2XeAKkIbAa6+irbkyz2jgREZGRpKXB52UTWymGMVu6C+SLMREWF0NjxqW9OcPLJrYmXhBYCVCtVTumqWVgIiJShxrTHoePzfHY+k5GZVM0Z55/zTOQ9RxmjGukMa1CiHVlQnN1hZ2qjRMRERlpcmmPsU0ZmjKVXp6wf4uExozP2KYMubT7Ym+vCSVANTalLcOe+nZMf5yIiEi9yZcjOvJlGlIuFvAcQ8p18JzKarCGlMu23jL5spbB15WuMqQ9KIa7j0l7lTgREZF6E4QRa7bk2dpbor0xRXtjprINvDVAzJbeEukteYLpUeWGmBAlQDXmGVtVD5CnVWAiIlKHSmHM+s4+SmGMMYa+chnbvy1GQ8qlHMZs6Kq8niQlQDVWiipFng273hLDUHm9pEVgIiJSh8IophjEdBUDgjDGdx2MY7C2UgSxL4jA+oSREqC6kvPNTpu/7chSyYJzvlaBiYhIfTIGgiimM9z5bmiMxQzDLVCToGsssoZ4D0ltHFfiRERE6o3nOhhj+kdDLBY7+KdD/yiJMXhusimJeoBqzMYx/av98JxKsjOwB4rjQBhDbCtxIiIi9SblGtK+g7WWloY0Kdf0J0SWcmTp7CuRSTmkXO0FVlc25wNc1+ACUX/yM/CIYnAB1zVszgfD2k4REZFaCGJozvoc0taAcaAcW8pRTDm2OA4c0tZAU8YnSLgfQD1ANdac8fAMlBk6D8ju8KdnKnEiIiL1JuUaRjekyXguFktnPiSMYzzHYVTOA2vIZbzEe4B0162xw8c0gIHdJbYxgOmPExERqTO+5zKhJcOmniLlMGJ0o08UW1zH4LuGlOcyrimD7yVbDVoJUI1ZCzZ+8Ro/NrZYlQESEZE6lEu5TB7dwMbuImu29bGuo0A5ikm5DpPbshw+tolD2xvIpZJNgDQHqMae3prf47hmEFfiRERE6k1lhZfhsfXdPL25lxiL70KM5anNvTy2vhvXqUyMTpJ6gGpsU1eRPRW3DONKnIiISL2J45iHn9nOlt4CpShiS76EjS3GMTRnfLb0Fli5djszxzXhOMn1y6gHqMY8d/fzfwbE/XEiIiL1ZnNPiQef7mBzT5muvpAotsQWotjS1RewuafM75/qYHNPKdF2KQGqsVza369xIiIiI8m2niJPbu6hq68MBlKuR9p3SbkeGOjqK/Pk5h629SQ7EqIhsBqrdllf0sv/REREktDVF7A9XybGEkYx3YWA2ILTvxlqZC3b82W6+pKth6cEqMY6q/xBq40TEREZSQJrCeKYQjHEGkO8w7LnQjnCWEtDxiNIeDm0EqAa29DZt1/jRERERpKs52BjKEbgmKFJTkRlPlAmrsQlSQlQjW3ri/ZrnIiIyEjiuxBGMTGwu7J4YRzjJ7wYSJOgayyOqhvaqjZORERkJOkuRETRi6+HjsKY7kKyHQFKgGrMrbKwU7VxIiIiI0lPMSDYw/SewFbikqQEqMYyfnWjjNXGiYiIjCT5cljVjgj5cphMg/opAaqx5obUfo0TEREZSeI4ZE/ru2x/XJKUANVYe666xKbaOBERkZGku1Td8vZq4/YXJUA1lstUN7RVbZyIiMhIUu3y9qSXwSsBqrFq89lk814REZFkNGd99rTC3e2PS5ISoBrLl6tb1ldtnIiIyEgyujG9xw2/PbcSlyQlQDVWCqpLbKqNExERGUma0y6pPVR6STmVuCQdEAnQjTfeyNSpU8lkMhx//PE8+OCDu4298847mT17Nq2treRyOWbNmsWtt946+HoQBHz84x/nmGOOIZfLMXHiRM4991zWr1+fxFfZSRBWN7hVbZyIiMhIsr0YY/eQANn+uCQNewJ0++23s3jxYq688koefvhhjj32WObPn8/mzZt3Gd/W1sZll13GihUrePTRR1m0aBGLFi3i3nvvBaCvr4+HH36Yyy+/nIcffpg777yTVatWcfrppyf5tQZVu8m7NoMXEZF6ZKOQ4h5WuBfDSlyShn3p0fXXX8/73vc+Fi1aBMCyZcu46667uPnmm7n00kt3ip87d+6Q5x/60If4+te/zgMPPMD8+fNpaWnh5z//+ZCY//7v/2bOnDmsXbuWQw89tGbfZVf2NO65t3EiIiIjyYaeMnva6N3aSlyShrUHqFwu89BDDzFv3rzBY47jMG/ePFasWLHH91trWb58OatWreLkk0/ebVxXVxfGGFpbW3f5eqlUoru7e8hjf7FVbnFRbZyIiMhIYm1lI9QXjemPS9KwJkBbt24liiLGjRs35Pi4cePYuHHjbt/X1dVFY2MjqVSK0047jRtuuIE3vOENu4wtFot8/OMf5x3veAfNzc27jFmyZAktLS2Dj8mTJ+/7l3qBtOfv8SI7/XEiIiL1JorsHku9xP1xSRr2OUD7oqmpiZUrV/LHP/6Ra6+9lsWLF3P//ffvFBcEAWeeeSbWWr785S/v9nyf+MQn6OrqGnw8++yz+62t41sz+Hu4yr5TiRMREak3zenqRjiqjdtfhnUOUHt7O67rsmnTpiHHN23axPjx43f7PsdxOPzwwwGYNWsWjz/+OEuWLBkyP2gg+XnmmWf45S9/udveH4B0Ok06XZv6A4e1ZXH28Js6phInIiJSbzb1VDe5udq4/WVYe4BSqRTHHXccy5cvHzwWxzHLly/nhBNOqPo8cRxTKpUGnw8kP08++SS/+MUvGD169H5t997Y3Fve8yZwphInIiJSb4phdXN7qo3bX4Z9FdjixYtZuHAhs2fPZs6cOSxdupR8Pj+4Kuzcc89l0qRJLFmyBKjM15k9ezbTp0+nVCpx9913c+uttw4OcQVBwNve9jYefvhhfvrTnxJF0eB8ora2NlKpZDcdfaajjz39pmFUias+5RMRERkZxrVk8Qy8WLk7z1TikjTsCdCCBQvYsmULV1xxBRs3bmTWrFncc889gxOj165di+M831GVz+e58MILWbduHdlslpkzZ3LbbbexYMECAJ577jl+/OMfA5XhsR3dd999Oy2jr7VCKSLeQxdQbCtxIiIi9eaEaa1kPUNPYDEM3fty4HnWN5wwrTXRdhlr97Q6/+DT3d1NS0sLXV1dLzp3qBrff3A1H73zsT3G/edbj+Zf50x7SZ8lIiJyoFm/Pc/ZN/2BNVsLu5wSYoBp7Vlue8/xTByVe0mftTf37xG5CmwkiePqenaqjRMRERlJOvsCWjIpmjLOTkmHAzRlHFoyKTr7gkTbNexDYPVuY3d1k5urjRMRERlJSlFMMYhozfqk3ZDeUoS1YAw0pV0yKY9iEFGKDrJJ0PVua76056C9iBMRERlJsq4hiGJ6SzEGg+s4gwlQZA29xRjfjckmvCmmEqAaS3nVjTJWGyciIjKSGGOwNqavHOAYQ8ZzcByIYygEEbG1tFoXk/CWUEqAamxic3XL+qqNExERGUliLOWostQ95bmUoxgbVnqAsp5LKQwpR5W4JCkBqrFDRlW3xUW1cSIiIiPJ9v7JzWnfo68cEkQ8PwQWxzSkvCFxSdG4S4119FVX2rvaOBERkZHEMQ4Y6C2FBCG4Dnhe5c8gqhzH9Mcl2a5EP+0gtK23usnN1caJiIiMJKOyLsVSSGwriY+1YOPKn55bKQZcKoeMyrqJtktDYDW2ta+65e3VxomIiIwkhSAitBYbQ2Qhgkr5ZwNu/3BYGFsKQbL18JQA1ZgXV1fXoNo4ERGRkWRrb8DA+q5gx3nOFmKeT0S29moOUF0JbHWXuNo4ERGRkcUShrtf42WBMIxBq8DqS9qrrq5BtXEiIiIjSXPapWwtEc8nHTHP98CEQGgtzelk5wCp26HGwqi6Mc1q40REREaSjmI0OAQ2MNnDMUOf2/64JKkHqMZ6S9Utb682TkREZCSJoggXg2sssa30+AyMdhnANeAaQ5RwR4ASoBrrK1eX2FQbJyIiMpKkPQ/jGOLQ4jB0po+hsgzeGEPaSzYlUQJUY45T3ShjtXEiIiIjyZhGHweLpTLk9cIEyAKOsYxp9BNtl+66NZbzq5vUVW2ciIjISFII7eCknxeu8xp8bkwlLkFKgGos5Ve5G3yVcSIiIiNJFEZE0YvXuovjmChMdg6Q7ro11luqrsBhtXEiIiIjSUchYE/rfIpBJS5JSoBqrtqMVsvgRUSk/kRhyJ5Gt0JbiUuSEqAay/rVTeqqNk5ERGQkWddZ3Wbf1cbtL0qAaqzaypZJV8AUERFJQiZVXapRbdz+ogSoxnqqLHBYbZyIiMhIMq4pvV/j9hclQDWWD6pb1ldtnIiIyEgyJuezp90uTX9ckpQA1ZipcnfbauNERERGkg09Eak9zPJIuZW4JCkBqrHGdHXFtquNExERGVksvuuQdtipJ8gAaQd894WbZNSe7ro1NrrKLr1q40REREaSQ9uypDyHMIxpTjtYwFowppIAlYKYlOdwaFs20XapB6jG8lUWOKw2TkREZCQZ35JlVEMKHEPYf6sz/V1BYQw4hlENKca3JJsAqQeoxrQKTEREDmYWw+Fjc3SXArryAcUdCj67DrTkfGaMzWH3OFV6/1ICVGORrW5Ms9o4ERGRkcQYQ2PaJ+26eG6IYyo7wxvAcQxp1yWX9jFGCVBdGZWt7hJXGyciIjKS5FIOG7uKhHHMpNYsQWSJrcUxBt8z9BRDNnWXyCVcCFF33RoL4up6dqqNExERGUm29pbpKQVkfZeGlEsYQxRXhr88B8LI0lMss7W3TGsuk1i7lADVmOtWt8VFtXEiIiIjSUe+jOM4NKY9NnYVKYYxMZVVWBnPYUxzBuM4dOTLibZLq8BqLOVUN6ZZbZyIiMhIkvIcwihmS75MObK4xuAbcI2hHFu25suEUWUpfJLUA1RjuUx1PTvVxomIiIwkh47KUAwiegsBbQ1pnB1ud3EEHX0lmtIuh45KbvgL1ANUc33l6ur7VBsnIiIyknT0hXiOg+8a+sKQKLLY2BJFlr4wxHcNnuPQ0ZdsORj1ANVYsVzdD1ptnIiIyEjSkS+T9V0Oac2yKV+ipxRircUYQ9p3mNCUxXPdxOcAKQGqMWur62SrNk5ERGQkSXkOrmtwHZeJzVmKYYyNY4zjkPEcPMeAMZoDVG/GNKf3a5yIiMhIMnV0A7mUy3OdRaaPyVEKYyILroG05/DUljyHjMowdXRDou1SAlRj09pzeMCLDXB5/XEiIiL1xnVdjprQzJaeEv/Y3Esm5eIZQ2gtxXJE2nOYOb458XIwGnepsantOVobXvxHHZVzmaoESERE6lAUW6a055g+ppGU61Aqx+TLIaVyTNpzmD6mkSntOaKECwKrB6jGjOMyaXSOYthDsWyJ4fk9UIBMyjCxLYdxtAxeRETqj+sYiuWIaWMaOWxMjvXdRUqBJe0bJrZksNZQCiLchOvhKQGqsZTnMLYxTRRatvUW6CpGxLHFcQyjMi6jGrOMbcokPvlLREQkOYa05zChNcv41uzgVhhpz2VDZwFs8sWAlQDVmGMM7Y1ptucD2hrTjG/1cLDEGMphSMZ3ac+lcBLeBVdERCQJUWwZ3ZjCLRi29pRpznpkPYcgjtnaU6Yx4zOqwdcQWL1pyng0pj1GNaZoNym6CiFRbEk5hnEtGaLY0pTxaMropxARkfrjuQ7NGZ/mjM/2vjJdfQG9NsQzhjFNKUY1pAbjEm1Xop92EDLG0JZL01UIyaUdJrSYwTlAYMmXYkY1pDHqARIRkTqUS7m0NabY2FVk+phGCkFEFFtcx5D1XTZ2F5nQmiGX0iqwuhLFlkmjskwf1wjWwWIwGCwGrMNhYxuZNCqbeNefiIhIEowxTBmdI5f22NhdxDGGhpSHYwwbu4vk0h6HtuUS7whQD1CN7dj1NyaXYktPiXJkSbmGMU1p2nLpwTgREZF61JL1efmkFp7Zlqejt0wYWzzHMKE1w6FtOVqyfuJtUgJUYzt2/R0+tolJoxoOiK4/ERGRJLVkfY6Z1EK+HBFGMZ7rkEu5wzYFRAlQjQ10/fUUQzZ2FxnVkCKdcgmieFi7/kRERJJmjKExfWCkHgdGK+rcgdj1JyIicjBTApSQA63rT0RE5GCmBChBB1LXn4iIyMFMS49ERETkoHNAJEA33ngjU6dOJZPJcPzxx/Pggw/uNvbOO+9k9uzZtLa2ksvlmDVrFrfeeuuQGGstV1xxBRMmTCCbzTJv3jyefPLJWn8NERERGSGGPQG6/fbbWbx4MVdeeSUPP/wwxx57LPPnz2fz5s27jG9ra+Oyyy5jxYoVPProoyxatIhFixZx7733DsZcd911fPGLX2TZsmX84Q9/IJfLMX/+fIrFYlJfS0RERA5gxlo7rCWIjz/+eF71qlfx3//93wDEcczkyZP54Ac/yKWXXlrVOV75yldy2mmncc0112CtZeLEiXz0ox/l4osvBqCrq4tx48Zxyy23cNZZZ+3xfN3d3bS0tNDV1UVzc/O+fzkRERFJzN7cv4e1B6hcLvPQQw8xb968wWOO4zBv3jxWrFixx/dba1m+fDmrVq3i5JNPBmD16tVs3LhxyDlbWlo4/vjjd3vOUqlEd3f3kIeIiIjUr2FNgLZu3UoURYwbN27I8XHjxrFx48bdvq+rq4vGxkZSqRSnnXYaN9xwA294wxsABt+3N+dcsmQJLS0tg4/Jkye/lK8lIiIiB7hhnwO0L5qamli5ciV//OMfufbaa1m8eDH333//Pp/vE5/4BF1dXYOPZ599dv81VkRERA44w1qUpr29Hdd12bRp05DjmzZtYvz48bt9n+M4HH744QDMmjWLxx9/nCVLljB37tzB923atIkJEyYMOeesWbN2eb50Ok06nX6J30ZERERGimHtAUqlUhx33HEsX7588FgcxyxfvpwTTjih6vPEcUypVAJg2rRpjB8/fsg5u7u7+cMf/rBX5xQREZH6NexliRcvXszChQuZPXs2c+bMYenSpeTzeRYtWgTAueeey6RJk1iyZAlQma8ze/Zspk+fTqlU4u677+bWW2/ly1/+MlCptvzhD3+YT3/608yYMYNp06Zx+eWXM3HiRN7ylrdU1aaBhXGaDC0iIjJyDNy3q1ngPuwJ0IIFC9iyZQtXXHEFGzduZNasWdxzzz2Dk5jXrl2L4zzfUZXP57nwwgtZt24d2WyWmTNnctttt7FgwYLBmI997GPk83nOP/98Ojs7Oemkk7jnnnvIZDJVtamnpwdAk6FFRERGoJ6eHlpaWl40ZtjrAB2I4jhm/fr1NDU17ffNSru7u5k8eTLPPvusagzVkK5zMnSdk6HrnAxd52TU8jpba+np6WHixIlDOk92Zdh7gA5EjuNwyCGH1PQzmpub9T+wBOg6J0PXORm6zsnQdU5Gra7znnp+BozIZfAiIiIiL4USIBERETnoKAFKWDqd5sorr1TdoRrTdU6GrnMydJ2ToeucjAPlOmsStIiIiBx01AMkIiIiBx0lQCIiInLQUQIkIiIiBx0lQCIiInLQUQJUAzfeeCNTp04lk8lw/PHH8+CDD75o/Pe+9z1mzpxJJpPhmGOO4e67706opSPb3lznr371q7zmNa9h1KhRjBo1innz5u3xd5GKvf33POA73/kOxpiq9+A72O3tde7s7OT9738/EyZMIJ1Oc8QRR+j/O6qwt9d56dKlHHnkkWSzWSZPnsxHPvIRisViQq0dmX7961/z5je/mYkTJ2KM4Yc//OEe33P//ffzyle+knQ6zeGHH84tt9xS83ZiZb/6zne+Y1OplL355pvt3/72N/u+973Ptra22k2bNu0y/re//a11Xdded9119rHHHrP//u//bn3ft3/5y18SbvnIsrfX+Z3vfKe98cYb7SOPPGIff/xxe95559mWlha7bt26hFs+suztdR6wevVqO2nSJPua17zG/su//EsyjR3B9vY6l0olO3v2bHvqqafaBx54wK5evdref//9duXKlQm3fGTZ2+v8zW9+06bTafvNb37Trl692t577712woQJ9iMf+UjCLR9Z7r77bnvZZZfZO++80wL2Bz/4wYvGP/3007ahocEuXrzYPvbYY/aGG26wruvae+65p6btVAK0n82ZM8e+//3vH3weRZGdOHGiXbJkyS7jzzzzTHvaaacNOXb88cfbf/u3f6tpO0e6vb3OLxSGoW1qarJf//rXa9XEurAv1zkMQ3viiSfa//3f/7ULFy5UAlSFvb3OX/7yl+1hhx1my+VyUk2sC3t7nd///vfb173udUOOLV682L761a+uaTvrSTUJ0Mc+9jH7spe9bMixBQsW2Pnz59ewZdZqCGw/KpfLPPTQQ8ybN2/wmOM4zJs3jxUrVuzyPStWrBgSDzB//vzdxsu+XecX6uvrIwgC2traatXMEW9fr/OnPvUpxo4dy3ve854kmjni7ct1/vGPf8wJJ5zA+9//fsaNG8fLX/5yPvOZzxBFUVLNHnH25TqfeOKJPPTQQ4PDZE8//TR33303p556aiJtPlgM131Qm6HuR1u3biWKIsaNGzfk+Lhx43jiiSd2+Z6NGzfuMn7jxo01a+dIty/X+YU+/vGPM3HixJ3+RyfP25fr/MADD3DTTTexcuXKBFpYH/blOj/99NP88pe/5Oyzz+buu+/mH//4BxdeeCFBEHDllVcm0ewRZ1+u8zvf+U62bt3KSSedhLWWMAy54IIL+OQnP5lEkw8au7sPdnd3UygUyGazNflc9QDJQeezn/0s3/nOd/jBD35AJpMZ7ubUjZ6eHs455xy++tWv0t7ePtzNqWtxHDN27Fi+8pWvcNxxx7FgwQIuu+wyli1bNtxNqyv3338/n/nMZ/jSl77Eww8/zJ133sldd93FNddcM9xNk/1APUD7UXt7O67rsmnTpiHHN23axPjx43f5nvHjx+9VvOzbdR7w+c9/ns9+9rP84he/4BWveEUtmzni7e11fuqpp1izZg1vfvObB4/FcQyA53msWrWK6dOn17bRI9C+/HueMGECvu/juu7gsaOOOoqNGzdSLpdJpVI1bfNItC/X+fLLL+ecc87hve99LwDHHHMM+Xye888/n8suuwzHUR/C/rC7+2Bzc3PNen9APUD7VSqV4rjjjmP58uWDx+I4Zvny5Zxwwgm7fM8JJ5wwJB7g5z//+W7jZd+uM8B1113HNddcwz333MPs2bOTaOqItrfXeebMmfzlL39h5cqVg4/TTz+d1772taxcuZLJkycn2fwRY1/+Pb/61a/mH//4x2CCCfD3v/+dCRMmKPnZjX25zn19fTslOQNJp9U2mvvNsN0HazrF+iD0ne98x6bTaXvLLbfYxx57zJ5//vm2tbXVbty40Vpr7TnnnGMvvfTSwfjf/va31vM8+/nPf94+/vjj9sorr9Qy+Crs7XX+7Gc/a1OplL3jjjvshg0bBh89PT3D9RVGhL29zi+kVWDV2dvrvHbtWtvU1GQ/8IEP2FWrVtmf/vSnduzYsfbTn/70cH2FEWFvr/OVV15pm5qa7Le//W379NNP25/97Gd2+vTp9swzzxyurzAi9PT02EceecQ+8sgjFrDXX3+9feSRR+wzzzxjrbX20ksvteecc85g/MAy+EsuucQ+/vjj9sYbb9Qy+JHqhhtusIceeqhNpVJ2zpw59ve///3ga6eccopduHDhkPjvfve79ogjjrCpVMq+7GUvs3fddVfCLR6Z9uY6T5kyxQI7Pa688srkGz7C7O2/5x0pAare3l7n3/3ud/b444+36XTaHnbYYfbaa6+1YRgm3OqRZ2+ucxAE9qqrrrLTp0+3mUzGTp482V544YV2+/btyTd8BLnvvvt2+f+3A9d24cKF9pRTTtnpPbNmzbKpVMoedthh9mtf+1rN22msVT+eiIiIHFw0B0hEREQOOkqARERE5KCjBEhEREQOOkqARERE5KCjBEhEREQOOkqARERE5KCjBEhEREQOOkqARET2kTGGH/7wh8PdDBHZB0qARGREWLFiBa7rctppp+3V+6ZOncrSpUtr0ygRGbGUAInIiHDTTTfxwQ9+kF//+tesX79+uJsjIiOcEiAROeD19vZy++238//9f/8fp512GrfccsuQ13/yk5/wqle9ikwmQ3t7O2eccQYAc+fO5ZlnnuEjH/kIxhiMMQBcddVVzJo1a8g5li5dytSpUwef//GPf+QNb3gD7e3ttLS0cMopp/Dwww/X8muKSIKUAInIAe+73/0uM2fO5Mgjj+Rd73oXN998MwPbGN51112cccYZnHrqqTzyyCMsX76cOXPmAHDnnXdyyCGH8KlPfYoNGzawYcOGqj+zp6eHhQsX8sADD/D73/+eGTNmcOqpp9LT01OT7ygiyfKGuwEiInty00038a53vQuAf/7nf6arq4tf/epXzJ07l2uvvZazzjqLq6++ejD+2GOPBaCtrQ3XdWlqamL8+PF79Zmve93rhjz/yle+QmtrK7/61a9405ve9BK/kYgMN/UAicgBbdWqVTz44IO84x3vAMDzPBYsWMBNN90EwMqVK3n961+/3z9306ZNvO9972PGjBm0tLTQ3NxMb28va9eu3e+fJSLJUw+QiBzQbrrpJsIwZOLEiYPHrLWk02n++7//m2w2u9fndBxncAhtQBAEQ54vXLiQbdu28YUvfIEpU6aQTqc54YQTKJfL+/ZFROSAoh4gETlghWHIN77xDf7zP/+TlStXDj7+/Oc/M3HiRL797W/zile8guXLl+/2HKlUiiiKhhwbM2YMGzduHJIErVy5ckjMb3/7Wy666CJOPfVUXvayl5FOp9m6det+/X4iMnzUAyQiB6yf/vSnbN++nfe85z20tLQMee1f//Vfuemmm/iP//gPXv/61zN9+nTOOusswjDk7rvv5uMf/zhQqQP061//mrPOOot0Ok17eztz585ly5YtXHfddbztbW/jnnvu4f/9v/9Hc3Pz4PlnzJjBrbfeyuzZs+nu7uaSSy7Zp94mETkwqQdIRA5YN910E/Pmzdsp+YFKAvSnP/2JtrY2vve97/HjH/+YWbNm8brXvY4HH3xwMO5Tn/oUa9asYfr06YwZMwaAo446ii996UvceOONHHvssTz44INcfPHFO3329u3beeUrX8k555zDRRddxNixY2v7hUUkMca+cCBcREREpM6pB0hEREQOOkqARERE5KCjBEhEREQOOkqARERE5KCjBEhEREQOOkqARERE5KCjBEhEREQOOkqARERE5KCjBEhEREQOOkqARERE5KCjBEhEREQOOkqARERE5KDz/wNU1qoDC+mA4gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(p_valid['label'],preds, alpha=0.2)\n",
    "plt.title('Validation Prediction Result')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Prediction')\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(epochs)\n",
    "plt.title('Validation Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,trainlosses, label='Train')\n",
    "plt.plot(x,vallosses, label='Validation')\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(epochs)\n",
    "plt.title('Validation Scores')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.plot(x,trainscores, label='Train')\n",
    "plt.plot(x,validscores, label='Validation')\n",
    "plt.savefig('score_plot.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------0start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.49265224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47557622\n",
      "Save first model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 1/5 [00:26<01:46, 26.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------1start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.48201624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47176582\n",
      "found better point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:53<01:19, 26.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------2start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.47180915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:11<00:45, 22.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47575274\n",
      "---------------3start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.4441313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [01:29<00:21, 21.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.4958575\n",
      "---------------4start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.373099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:47<00:00, 21.59s/it]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.52592385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------0start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.48821443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47471055\n",
      "Save first model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 1/5 [00:26<01:47, 26.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------1start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.48364976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47453502\n",
      "found better point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:53<01:20, 26.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------2start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.47842118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:11<00:45, 22.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47469148\n",
      "---------------3start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.47856256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [01:30<00:21, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47454026\n",
      "---------------4start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.47949943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:48<00:00, 21.71s/it]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.4747848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------0start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.49064898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47284624\n",
      "Save first model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 1/5 [00:26<01:46, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------1start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.48253953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47272274\n",
      "found better point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:53<01:20, 26.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------2start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.47232613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:11<00:45, 22.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47283256\n",
      "---------------3start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.45910183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [01:30<00:21, 21.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47705802\n",
      "---------------4start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.42127535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:48<00:00, 21.70s/it]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.4988171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------0start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.5086097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47346494\n",
      "Save first model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 1/5 [00:26<01:46, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------1start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.47991726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.47125748\n",
      "found better point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:53<01:20, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------2start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.4739972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISMTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.4708853\n",
      "found better point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:19<00:53, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------3start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.46082142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [01:38<00:23, 23.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.48338047\n",
      "---------------4start-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainscore is 0.40872478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:56<00:00, 23.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valscore is 0.50322455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bestscores = []\n",
    "bestscores.append(bestscore)\n",
    "\n",
    "for fold in range(1,5):\n",
    "    \n",
    "    # initializing the data\n",
    "    p_train = train[train[\"kfold\"]!=fold].reset_index(drop=True)\n",
    "    p_valid = train[train[\"kfold\"]==fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = BERTDataSet(p_train[\"text\"],p_train[\"label\"])\n",
    "    valid_dataset = BERTDataSet(p_valid[\"text\"],p_valid[\"label\"])\n",
    "    \n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1)\n",
    "    \n",
    "    model.to(device)\n",
    "    LR=2e-5\n",
    "    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n",
    "    train_steps = int(len(p_train)/train_batch*epochs)\n",
    "    num_steps = int(train_steps*0.1)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n",
    "\n",
    "    trainlosses = []\n",
    "    vallosses = []\n",
    "    bestscore = None\n",
    "    trainscores = []\n",
    "    validscores = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        print(\"---------------\" + str(epoch) + \"start-------------\")\n",
    "\n",
    "        trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)\n",
    "        trainlosses.append(trainloss)\n",
    "        trainscores.append(trainscore)\n",
    "\n",
    "        print(\"trainscore is \" + str(trainscore))\n",
    "\n",
    "        preds,validloss,valscore=validating(valid_dataloader,model)\n",
    "        vallosses.append(validloss)\n",
    "        validscores.append(valscore)\n",
    "\n",
    "        print(\"valscore is \" + str(valscore))\n",
    "\n",
    "        if bestscore is None:\n",
    "            bestscore = valscore\n",
    "\n",
    "            print(\"Save first model\")\n",
    "\n",
    "            state = {\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'optimizer_dict': optimizer.state_dict(),\n",
    "                            \"bestscore\":bestscore\n",
    "                        }\n",
    "\n",
    "            torch.save(state, \"model\" + str(fold) + \".pth\") \n",
    "\n",
    "        elif bestscore > valscore:\n",
    "            bestscore = valscore\n",
    "            print(\"found better point\")\n",
    "\n",
    "            state = {\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'optimizer_dict': optimizer.state_dict(),\n",
    "                            \"bestscore\":bestscore\n",
    "                        }\n",
    "            torch.save(state, \"model\"+ str(fold) + \".pth\")\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    bestscores.append(bestscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My CV is 0.47258273.\n"
     ]
    }
   ],
   "source": [
    "np.mean(bestscores)\n",
    "print(\"My CV is \" + str(np.mean(bestscores))+ \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predicting(test_dataloader,model):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()   \n",
    "    allpreds = []\n",
    "    preds = []\n",
    "    allvalloss=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for a in test_dataloader:\n",
    "\n",
    "            ids = a[\"ids\"].to(device)\n",
    "            mask = a[\"mask\"].to(device)\n",
    "\n",
    "            output = model(ids,mask)\n",
    "            output = output[\"logits\"].squeeze(-1)\n",
    "            preds.append(output.cpu().numpy())\n",
    "\n",
    "        preds = np.concatenate(preds)\n",
    "        allpreds.append(preds)\n",
    "\n",
    "    return allpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "tpreds = predicting(test_dataloader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_pred = []\n",
    "for p in tpreds[0]:\n",
    "    test_pred+=[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.080536</td>\n",
       "      <td>0.271183</td>\n",
       "      <td>0.648281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.095635</td>\n",
       "      <td>0.114152</td>\n",
       "      <td>0.790213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.483301</td>\n",
       "      <td>0.369866</td>\n",
       "      <td>0.146834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.080536        0.271183    0.648281\n",
       "1   211333        0.095635        0.114152    0.790213\n",
       "2  1233961        0.483301        0.369866    0.146834"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submit=pd.read_csv('dataset/sample_submission.csv')\n",
    "pa=test_pred[0:len(TEST)//2]\n",
    "pb=test_pred[len(TEST)//2:]\n",
    "pc=[]\n",
    "for i in range(len(TEST)//2):\n",
    "    pc+=[np.clip(1-(pa[i]+pb[i]),0,1)]\n",
    "submit['winner_model_a']=pa\n",
    "submit['winner_model_b']=pb\n",
    "submit['winner_tie']=pc\n",
    "display(submit)\n",
    "submit.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-p310-torch210-srihari",
   "language": "python",
   "name": "venv-p310-torch251-srihari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
